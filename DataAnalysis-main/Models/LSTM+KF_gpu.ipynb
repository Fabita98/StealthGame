{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-07T12:10:19.283660Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ax.service.managed_loop import optimize\n",
    "from statistics import mean\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "import logging\n",
    "from ax.utils.common.logger import get_logger\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "# Only show log messages of ERROR while testing.\n",
    "logger = get_logger(__name__, level=logging.ERROR)\n",
    "if logger.parent is not None and hasattr(logger.parent, \"handlers\"):\n",
    "    logger.parent.handlers[0].setLevel(logging.ERROR)\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('OCULUS_dataset.csv')\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "features = data.drop([\"game_section\", \"stress_label\", \"subject\"], axis=1)\n",
    "labels = data[\"stress_label\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "labels_tensor = torch.tensor(labels.values, dtype=torch.float32).to(device)\n",
    "\n",
    "class StressLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate_1, dropout_rate_2, dropout_rate_fc):\n",
    "        super(StressLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate_1)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate_2)\n",
    "        self.dropout = nn.Dropout(dropout_rate_fc)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "parameters = [\n",
    "    {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-6, 1e-3], \"log_scale\": True, \"value_type\": \"float\"},\n",
    "    {\"name\": \"hidden_size\", \"type\": \"range\", \"bounds\": [50, 256], \"value_type\": \"int\"},\n",
    "    {\"name\": \"num_layers\", \"type\": \"range\", \"bounds\": [2, 3], \"value_type\": \"int\"},\n",
    "    {\"name\": \"num_epochs\", \"type\": \"fixed\", \"value\": 150, \"value_type\": \"int\"},\n",
    "    {\"name\": \"dropout_rate_1\", \"type\": \"range\", \"bounds\": [0.0, 0.7], \"value_type\": \"float\"},\n",
    "    {\"name\": \"dropout_rate_2\", \"type\": \"range\", \"bounds\": [0.0, 0.7], \"value_type\": \"float\"},\n",
    "    {\"name\": \"dropout_rate_fc\", \"type\": \"range\", \"bounds\": [0.0, 0.7], \"value_type\": \"float\"},\n",
    "    {\"name\": \"weight_decay\", \"type\": \"range\", \"bounds\": [1e-6, 1], \"value_type\": \"float\"},\n",
    "    {\"name\": \"batch_size\", \"type\": \"range\", \"bounds\": [10, 100], \"value_type\": \"int\"}\n",
    "]\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "mae_criterion = nn.L1Loss().to(device)\n",
    "\n",
    "def evaluate_full_dataset(parameters, features, labels):\n",
    "    print(\"Evaluating with parameters: \" + str(parameters))\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=int(parameters['batch_size']), shuffle=False)\n",
    "\n",
    "    # Model initialization\n",
    "    model = StressLSTM(input_size=features.shape[1],\n",
    "                       hidden_size=int(parameters['hidden_size']),\n",
    "                       num_layers=int(parameters['num_layers']),\n",
    "                       dropout_rate_1=parameters['dropout_rate_1'],\n",
    "                       dropout_rate_2=parameters['dropout_rate_2'],\n",
    "                       dropout_rate_fc=parameters['dropout_rate_fc']).to(device)\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=parameters['lr'], weight_decay=parameters['weight_decay'])\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience, patience_threshold = 0, 10  # Early stopping\n",
    "\n",
    "    for epoch in range(int(parameters['num_epochs'])):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for batch_features, batch_labels in train_dataloader:\n",
    "            batch_features, batch_labels = batch_features.unsqueeze(0).to(device), batch_labels.view(-1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val.unsqueeze(0).to(device))\n",
    "            val_loss = criterion(val_outputs.view(-1), y_val.view(-1).to(device))\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{int(parameters[\"num_epochs\"])}], Loss: {running_loss/len(train_dataloader)}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= patience_threshold:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "def evaluate_with_objective(parameters):\n",
    "    mse_loss = evaluate_full_dataset(parameters, features_tensor, labels_tensor)\n",
    "    print(f\"MSE Loss: {mse_loss}\")\n",
    "    return mse_loss  # Return only MSE as the optimization target\n",
    "\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters,\n",
    "    evaluation_function=evaluate_with_objective,\n",
    "    objective_name='loss'\n",
    ")\n",
    "\n",
    "print(best_parameters)\n",
    "\n",
    "input_size = features.shape[1]\n",
    "output_size = 1\n",
    "learning_rate = best_parameters['lr']\n",
    "hidden_size = int(best_parameters['hidden_size'])\n",
    "num_layers = int(best_parameters['num_layers'])\n",
    "num_epochs = int(best_parameters['num_epochs'])\n",
    "dropout_rate_1 = best_parameters['dropout_rate_1']\n",
    "dropout_rate_2 = best_parameters['dropout_rate_2']\n",
    "dropout_rate_fc = best_parameters['dropout_rate_fc']\n",
    "weight_decay = best_parameters['weight_decay']\n",
    "batch_size = int(best_parameters['batch_size'])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1)\n",
    "\n",
    "def calculate_loss_and_mae(model, criterion, mae_criterion, inputs, targets, use_kalman_filter=False):\n",
    "    outputs = model(inputs.unsqueeze(0).to(device))\n",
    "    if use_kalman_filter:\n",
    "        (filtered_state_means, filtered_state_covariances) = kf.filter(outputs.cpu().detach().numpy())\n",
    "        outputs = torch.tensor(filtered_state_means, dtype=torch.float32).view(1, -1).to(device)\n",
    "    loss = criterion(outputs, targets.unsqueeze(0).to(device))\n",
    "    mae = mae_criterion(outputs, targets.unsqueeze(0).to(device))\n",
    "    return loss, mae\n",
    "\n",
    "grouped = data.groupby('subject')\n",
    "\n",
    "def do_stuff():\n",
    "    all_X_train, all_y_train, all_X_val, all_y_val, all_X_test, all_y_test = [], [], [], [], [], []\n",
    "    for name, group in grouped:\n",
    "        print(f\"Subject: {name}\")\n",
    "        features = group.drop([\"game_section\", \"stress_label\", \"subject\"], axis=1)\n",
    "        labels = group[\"stress_label\"]\n",
    "    \n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "    \n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "        labels_tensor = torch.tensor(labels.values, dtype=torch.float32).to(device)\n",
    "    \n",
    "        for train_index, test_index in tscv.split(features_tensor):\n",
    "            X_train_val, X_test = features_tensor[train_index], features_tensor[test_index]\n",
    "            y_train_val, y_test = labels_tensor[train_index], labels_tensor[test_index]\n",
    "    \n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, shuffle=False)\n",
    "    \n",
    "            all_X_train.append(X_train)\n",
    "            all_y_train.append(y_train)\n",
    "            all_X_val.append(X_val)\n",
    "            all_y_val.append(y_val)\n",
    "            all_X_test.append(X_test)\n",
    "            all_y_test.append(y_test)\n",
    "    \n",
    "    X_train = torch.cat(all_X_train).to(device)\n",
    "    y_train = torch.cat(all_y_train).to(device)\n",
    "    X_val = torch.cat(all_X_val).to(device)\n",
    "    y_val = torch.cat(all_y_val).to(device)\n",
    "    X_test = torch.cat(all_X_test).to(device)\n",
    "    y_test = torch.cat(all_y_test).to(device)\n",
    "    \n",
    "    window_sizes = [2, 4, 6]\n",
    "    \n",
    "    all_train_losses, all_val_losses, all_train_maes, all_val_maes, losses, maes = [], [], [], [], [], []\n",
    "    all_filtered_train_losses, all_filtered_val_losses, all_filtered_train_maes, all_filtered_val_maes, filtered_losses, filtered_maes = [], [], [], [], [], []\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        print(f\"Window Size: {window_size}\")\n",
    "        if len(X_train) < window_size or len(y_train) < window_size:\n",
    "            continue\n",
    "    \n",
    "        train_dataset = TensorDataset(X_train[:len(X_train)//window_size*window_size], y_train[:len(y_train)//window_size*window_size])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=window_size, shuffle=False)\n",
    "    \n",
    "        model = StressLSTM(input_size, hidden_size, num_layers, dropout_rate_1, dropout_rate_2, dropout_rate_fc).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            training_outputs = model(X_train.unsqueeze(0).to(device))\n",
    "            kf.em(training_outputs.cpu().detach().numpy(), n_iter=10)\n",
    "    \n",
    "        train_losses, val_losses, train_maes, val_maes = [], [], [], []\n",
    "        filtered_train_losses, filtered_val_losses, filtered_train_maes, filtered_val_maes = [], [], [], []\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\", end=\"\\r\")\n",
    "            running_loss = 0.0\n",
    "            running_mae = 0.0\n",
    "            running_filtered_loss = 0.0\n",
    "            running_filtered_mae = 0.0\n",
    "            num_batches = 0\n",
    "    \n",
    "            for batch_features, batch_labels in train_dataloader:\n",
    "                model.train()\n",
    "                loss, mae = calculate_loss_and_mae(model, criterion, mae_criterion, batch_features, batch_labels)\n",
    "                filtered_loss, filtered_mae = calculate_loss_and_mae(model, criterion, mae_criterion, batch_features, batch_labels, use_kalman_filter=True)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                running_loss += loss.item()\n",
    "                running_mae += mae.item()\n",
    "                running_filtered_loss += filtered_loss.item()\n",
    "                running_filtered_mae += filtered_mae.item()\n",
    "                num_batches += 1\n",
    "    \n",
    "            average_train_loss = running_loss / num_batches\n",
    "            average_train_mae = running_mae / num_batches\n",
    "            average_filtered_loss = running_filtered_loss / num_batches\n",
    "            average_filtered_mae = running_filtered_mae / num_batches\n",
    "            train_losses.append(average_train_loss)\n",
    "            train_maes.append(average_train_mae)\n",
    "            filtered_train_losses.append(average_filtered_loss)\n",
    "            filtered_train_maes.append(average_filtered_mae)\n",
    "    \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_mae = calculate_loss_and_mae(model, criterion, mae_criterion, X_val, y_val)\n",
    "                val_filtered_loss, val_filtered_mae = calculate_loss_and_mae(model, criterion, mae_criterion, X_val, y_val, use_kalman_filter=True)\n",
    "    \n",
    "            val_losses.append(val_loss.item())\n",
    "            val_maes.append(val_mae.item())\n",
    "            filtered_val_losses.append(val_filtered_loss.item())\n",
    "            filtered_val_maes.append(val_filtered_mae.item())\n",
    "    \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss, test_mae = calculate_loss_and_mae(model, criterion, mae_criterion, X_test, y_test)\n",
    "                test_filtered_loss, test_filtered_mae = calculate_loss_and_mae(model, criterion, mae_criterion, X_test, y_test, use_kalman_filter=True)\n",
    "    \n",
    "            losses.append(test_loss.item())\n",
    "            maes.append(test_mae.item())\n",
    "            filtered_losses.append(test_filtered_loss.item())\n",
    "            filtered_maes.append(test_filtered_mae.item())\n",
    "    \n",
    "        fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        maxy = max(max(train_losses), max(val_losses), max(filtered_train_losses), max(filtered_val_losses), max(train_maes), max(val_maes), max(filtered_train_maes), max(filtered_val_maes))\n",
    "    \n",
    "        axs[0, 0].plot(train_losses, label='Training MSE')\n",
    "        axs[0, 0].plot(val_losses, label='Validation MSE')\n",
    "        axs[0, 0].set_xlabel('Epoch')\n",
    "        axs[0, 0].set_ylabel('MSE')\n",
    "        axs[0, 0].set_ylim(0, maxy)\n",
    "        axs[0, 0].legend()\n",
    "        axs[0, 0].set_title('Training and Validation MSE')\n",
    "    \n",
    "        axs[0, 1].plot(filtered_train_losses, label='Filtered Training MSE')\n",
    "        axs[0, 1].plot(filtered_val_losses, label='Filtered Validation MSE')\n",
    "        axs[0, 1].set_xlabel('Epoch')\n",
    "        axs[0, 1].set_ylabel('MSE')\n",
    "        axs[0, 1].set_ylim(0, maxy)\n",
    "        axs[0, 1].legend()\n",
    "        axs[0, 1].set_title('Filtered Training and Validation MSE')\n",
    "    \n",
    "        axs[1, 0].plot(train_maes, label='Training MAE')\n",
    "        axs[1, 0].plot(val_maes, label='Validation MAE')\n",
    "        axs[1, 0].set_xlabel('Epoch')\n",
    "        axs[1, 0].set_ylabel('MAE')\n",
    "        axs[1, 0].set_ylim(0, maxy)\n",
    "        axs[1, 0].legend()\n",
    "        axs[1, 0].set_title('Training and Validation MAE')\n",
    "    \n",
    "        axs[1, 1].plot(filtered_train_maes, label='Filtered Training MAE')\n",
    "        axs[1, 1].plot(filtered_val_maes, label='Filtered Validation MAE')\n",
    "        axs[1, 1].set_xlabel('Epoch')\n",
    "        axs[1, 1].set_ylabel('MAE')\n",
    "        axs[1, 1].set_ylim(0, maxy)\n",
    "        axs[1, 1].legend()\n",
    "        axs[1, 1].set_title('Filtered Training and Validation MAE')\n",
    "    \n",
    "        fig.suptitle(f'Window Size: {window_size}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "        print(\"Average test MSE :\", mean(losses))\n",
    "        print(\"Average test MAE :\", mean(maes))\n",
    "    \n",
    "        print(\"Average filtered test MSE :\", mean(filtered_losses))\n",
    "        print(\"Average filtered test MAE :\", mean(filtered_maes))\n",
    "        \n",
    "for i in range(10):\n",
    "    do_stuff()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with parameters: {'lr': 1.7200460023147145e-06, 'hidden_size': 246, 'num_layers': 3, 'dropout_rate_1': 0.5831521153450012, 'dropout_rate_2': 0.6510676383972167, 'dropout_rate_fc': 0.36499016284942626, 'weight_decay': 0.43428298778331276, 'batch_size': 85, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([85])) that is different to the input size (torch.Size([1, 85])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([1, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.38639323388115837, Val Loss: 0.2935720980167389\n",
      "Epoch [2/150], Loss: 0.38563668489519315, Val Loss: 0.2930510342121124\n",
      "Epoch [3/150], Loss: 0.38521109109397156, Val Loss: 0.2925291955471039\n",
      "Epoch [4/150], Loss: 0.384090798417643, Val Loss: 0.2920148968696594\n",
      "Epoch [5/150], Loss: 0.38378547341957436, Val Loss: 0.2915027141571045\n",
      "Epoch [6/150], Loss: 0.38269740917672546, Val Loss: 0.29099616408348083\n",
      "Epoch [7/150], Loss: 0.3818275301196313, Val Loss: 0.29049786925315857\n",
      "Epoch [8/150], Loss: 0.38060208078536945, Val Loss: 0.2900025546550751\n",
      "Epoch [9/150], Loss: 0.38003199320223374, Val Loss: 0.28951606154441833\n",
      "Epoch [10/150], Loss: 0.37940295454177814, Val Loss: 0.28903499245643616\n",
      "Epoch [11/150], Loss: 0.3782068853092901, Val Loss: 0.28855812549591064\n",
      "Epoch [12/150], Loss: 0.3776259412673318, Val Loss: 0.2880879044532776\n",
      "Epoch [13/150], Loss: 0.37703609996933046, Val Loss: 0.28762325644493103\n",
      "Epoch [14/150], Loss: 0.37612651260096136, Val Loss: 0.28717005252838135\n",
      "Epoch [15/150], Loss: 0.375420156740031, Val Loss: 0.28671795129776\n",
      "Epoch [16/150], Loss: 0.374867518385083, Val Loss: 0.2862723469734192\n",
      "Epoch [17/150], Loss: 0.37385148976503285, Val Loss: 0.2858327627182007\n",
      "Epoch [18/150], Loss: 0.3735209827832246, Val Loss: 0.28539738059043884\n",
      "Epoch [19/150], Loss: 0.37245762125604737, Val Loss: 0.2849692106246948\n",
      "Epoch [20/150], Loss: 0.3718294128617745, Val Loss: 0.28454694151878357\n",
      "Epoch [21/150], Loss: 0.37139819631889714, Val Loss: 0.28412848711013794\n",
      "Epoch [22/150], Loss: 0.37005272204593076, Val Loss: 0.2837125360965729\n",
      "Epoch [23/150], Loss: 0.3696306880279365, Val Loss: 0.28330346941947937\n",
      "Epoch [24/150], Loss: 0.3689892840398065, Val Loss: 0.2828981578350067\n",
      "Epoch [25/150], Loss: 0.36846949441074317, Val Loss: 0.2824992835521698\n",
      "Epoch [26/150], Loss: 0.3681155779414763, Val Loss: 0.28210440278053284\n",
      "Epoch [27/150], Loss: 0.36657061251038214, Val Loss: 0.28171420097351074\n",
      "Epoch [28/150], Loss: 0.36624757670876334, Val Loss: 0.2813292443752289\n",
      "Epoch [29/150], Loss: 0.3655261886385033, Val Loss: 0.2809484004974365\n",
      "Epoch [30/150], Loss: 0.36513266005253386, Val Loss: 0.2805700898170471\n",
      "Epoch [31/150], Loss: 0.36448978146506567, Val Loss: 0.2801959812641144\n",
      "Epoch [32/150], Loss: 0.3638367573651722, Val Loss: 0.2798275351524353\n",
      "Epoch [33/150], Loss: 0.36336178597757374, Val Loss: 0.27946221828460693\n",
      "Epoch [34/150], Loss: 0.3625740062887386, Val Loss: 0.2791002690792084\n",
      "Epoch [35/150], Loss: 0.3619753565576117, Val Loss: 0.27874255180358887\n",
      "Epoch [36/150], Loss: 0.3610274821393571, Val Loss: 0.27839067578315735\n",
      "Epoch [37/150], Loss: 0.3605583582262872, Val Loss: 0.27804258465766907\n",
      "Epoch [38/150], Loss: 0.36032985902186165, Val Loss: 0.2776959240436554\n",
      "Epoch [39/150], Loss: 0.3592078910225024, Val Loss: 0.27735278010368347\n",
      "Epoch [40/150], Loss: 0.35886588837888284, Val Loss: 0.2770165205001831\n",
      "Epoch [41/150], Loss: 0.3579276308606742, Val Loss: 0.27668046951293945\n",
      "Epoch [42/150], Loss: 0.3580442733936391, Val Loss: 0.2763501703739166\n",
      "Epoch [43/150], Loss: 0.3574241721465931, Val Loss: 0.27602386474609375\n",
      "Epoch [44/150], Loss: 0.35687820834376044, Val Loss: 0.27569976449012756\n",
      "Epoch [45/150], Loss: 0.3557938240721064, Val Loss: 0.27537912130355835\n",
      "Epoch [46/150], Loss: 0.3555067146544234, Val Loss: 0.2750610411167145\n",
      "Epoch [47/150], Loss: 0.3548935415002249, Val Loss: 0.27474623918533325\n",
      "Epoch [48/150], Loss: 0.35442065864296285, Val Loss: 0.2744350731372833\n",
      "Epoch [49/150], Loss: 0.3536548335105181, Val Loss: 0.2741282880306244\n",
      "Epoch [50/150], Loss: 0.35349119069465135, Val Loss: 0.27382269501686096\n",
      "Epoch [51/150], Loss: 0.35251647691731736, Val Loss: 0.27352210879325867\n",
      "Epoch [52/150], Loss: 0.35230436239202145, Val Loss: 0.2732236683368683\n",
      "Epoch [53/150], Loss: 0.3518772014748242, Val Loss: 0.2729293406009674\n",
      "Epoch [54/150], Loss: 0.35097165374180017, Val Loss: 0.2726365923881531\n",
      "Epoch [55/150], Loss: 0.35087958622281834, Val Loss: 0.27234670519828796\n",
      "Epoch [56/150], Loss: 0.3501124855888597, Val Loss: 0.272059828042984\n",
      "Epoch [57/150], Loss: 0.34971110841606634, Val Loss: 0.27177533507347107\n",
      "Epoch [58/150], Loss: 0.34940607223849174, Val Loss: 0.271494597196579\n",
      "Epoch [59/150], Loss: 0.348805827577993, Val Loss: 0.2712169587612152\n",
      "Epoch [60/150], Loss: 0.3481057034331863, Val Loss: 0.27094143629074097\n",
      "Epoch [61/150], Loss: 0.34788333245758285, Val Loss: 0.2706693112850189\n",
      "Epoch [62/150], Loss: 0.347467983078401, Val Loss: 0.2703991234302521\n",
      "Epoch [63/150], Loss: 0.3467693622145107, Val Loss: 0.27013063430786133\n",
      "Epoch [64/150], Loss: 0.34647451293796805, Val Loss: 0.26986628770828247\n",
      "Epoch [65/150], Loss: 0.3458481846471964, Val Loss: 0.26960307359695435\n",
      "Epoch [66/150], Loss: 0.3452263151816392, Val Loss: 0.26934367418289185\n",
      "Epoch [67/150], Loss: 0.3449314463315374, Val Loss: 0.26908648014068604\n",
      "Epoch [68/150], Loss: 0.34453381307549397, Val Loss: 0.2688313126564026\n",
      "Epoch [69/150], Loss: 0.343925982177005, Val Loss: 0.2685808837413788\n",
      "Epoch [70/150], Loss: 0.3436357859458964, Val Loss: 0.2683298587799072\n",
      "Epoch [71/150], Loss: 0.34305558483100546, Val Loss: 0.2680825889110565\n",
      "Epoch [72/150], Loss: 0.34275776741363234, Val Loss: 0.2678386867046356\n",
      "Epoch [73/150], Loss: 0.3422122432581954, Val Loss: 0.26759523153305054\n",
      "Epoch [74/150], Loss: 0.3416816794266135, Val Loss: 0.26735422015190125\n",
      "Epoch [75/150], Loss: 0.34126835575295705, Val Loss: 0.26711592078208923\n",
      "Epoch [76/150], Loss: 0.34065286126934874, Val Loss: 0.26688075065612793\n",
      "Epoch [77/150], Loss: 0.3404369288897615, Val Loss: 0.2666469216346741\n",
      "Epoch [78/150], Loss: 0.3400981241868714, Val Loss: 0.2664145529270172\n",
      "Epoch [79/150], Loss: 0.339722133257379, Val Loss: 0.26618507504463196\n",
      "Epoch [80/150], Loss: 0.3395955895657762, Val Loss: 0.26595914363861084\n",
      "Epoch [81/150], Loss: 0.33889218748120936, Val Loss: 0.2657339572906494\n",
      "Epoch [82/150], Loss: 0.3386217331103349, Val Loss: 0.26551106572151184\n",
      "Epoch [83/150], Loss: 0.33809215607026877, Val Loss: 0.2652892470359802\n",
      "Epoch [84/150], Loss: 0.3375706163124513, Val Loss: 0.2650696039199829\n",
      "Epoch [85/150], Loss: 0.3371172729500775, Val Loss: 0.2648519277572632\n",
      "Epoch [86/150], Loss: 0.3368989169218783, Val Loss: 0.2646367847919464\n",
      "Epoch [87/150], Loss: 0.3363602417442253, Val Loss: 0.2644234597682953\n",
      "Epoch [88/150], Loss: 0.3360326692651389, Val Loss: 0.2642110586166382\n",
      "Epoch [89/150], Loss: 0.3353263166875152, Val Loss: 0.26400262117385864\n",
      "Epoch [90/150], Loss: 0.3351123297302905, Val Loss: 0.2637956738471985\n",
      "Epoch [91/150], Loss: 0.3344112881222519, Val Loss: 0.26358887553215027\n",
      "Epoch [92/150], Loss: 0.33429184417098257, Val Loss: 0.2633846402168274\n",
      "Epoch [93/150], Loss: 0.33411974049473214, Val Loss: 0.26318299770355225\n",
      "Epoch [94/150], Loss: 0.3339522968093723, Val Loss: 0.2629827558994293\n",
      "Epoch [95/150], Loss: 0.3332634073508493, Val Loss: 0.2627844512462616\n",
      "Epoch [96/150], Loss: 0.3326789513546026, Val Loss: 0.26258811354637146\n",
      "Epoch [97/150], Loss: 0.33247276480799004, Val Loss: 0.262393981218338\n",
      "Epoch [98/150], Loss: 0.3319523489197432, Val Loss: 0.2622002363204956\n",
      "Epoch [99/150], Loss: 0.33216725513970446, Val Loss: 0.2620095908641815\n",
      "Epoch [100/150], Loss: 0.3317866326956931, Val Loss: 0.26182135939598083\n",
      "Epoch [101/150], Loss: 0.33113691103407894, Val Loss: 0.2616344392299652\n",
      "Epoch [102/150], Loss: 0.3308373575051457, Val Loss: 0.2614471912384033\n",
      "Epoch [103/150], Loss: 0.3302893284392559, Val Loss: 0.26126229763031006\n",
      "Epoch [104/150], Loss: 0.32989564032877905, Val Loss: 0.26107993721961975\n",
      "Epoch [105/150], Loss: 0.3298485693285021, Val Loss: 0.26089978218078613\n",
      "Epoch [106/150], Loss: 0.3297249542833385, Val Loss: 0.26072046160697937\n",
      "Epoch [107/150], Loss: 0.328936957296426, Val Loss: 0.2605433166027069\n",
      "Epoch [108/150], Loss: 0.32878388072979653, Val Loss: 0.26036614179611206\n",
      "Epoch [109/150], Loss: 0.32844579443966937, Val Loss: 0.26019009947776794\n",
      "Epoch [110/150], Loss: 0.3280145345666146, Val Loss: 0.26001572608947754\n",
      "Epoch [111/150], Loss: 0.327634134029938, Val Loss: 0.2598440945148468\n",
      "Epoch [112/150], Loss: 0.3276806386598086, Val Loss: 0.2596736252307892\n",
      "Epoch [113/150], Loss: 0.32699922958420496, Val Loss: 0.25950515270233154\n",
      "Epoch [114/150], Loss: 0.3269648079334174, Val Loss: 0.25933897495269775\n",
      "Epoch [115/150], Loss: 0.32695804214326, Val Loss: 0.2591744065284729\n",
      "Epoch [116/150], Loss: 0.32629081092269746, Val Loss: 0.25901126861572266\n",
      "Epoch [117/150], Loss: 0.32575094763000134, Val Loss: 0.25884929299354553\n",
      "Epoch [118/150], Loss: 0.3256260404457985, Val Loss: 0.25868919491767883\n",
      "Epoch [119/150], Loss: 0.3251716176016351, Val Loss: 0.25852859020233154\n",
      "Epoch [120/150], Loss: 0.32501304190669017, Val Loss: 0.25837066769599915\n",
      "Epoch [121/150], Loss: 0.32474196443365794, Val Loss: 0.25821465253829956\n",
      "Epoch [122/150], Loss: 0.32437461788245175, Val Loss: 0.2580588459968567\n",
      "Epoch [123/150], Loss: 0.32399623784220827, Val Loss: 0.25790414214134216\n",
      "Epoch [124/150], Loss: 0.3238687590396, Val Loss: 0.2577511668205261\n",
      "Epoch [125/150], Loss: 0.32334598598970193, Val Loss: 0.25760018825531006\n",
      "Epoch [126/150], Loss: 0.3230102166722892, Val Loss: 0.2574498653411865\n",
      "Epoch [127/150], Loss: 0.3228432377642494, Val Loss: 0.2573022246360779\n",
      "Epoch [128/150], Loss: 0.32253710515165734, Val Loss: 0.25715482234954834\n",
      "Epoch [129/150], Loss: 0.3222681644483138, Val Loss: 0.2570093870162964\n",
      "Epoch [130/150], Loss: 0.32199676082295886, Val Loss: 0.2568642795085907\n",
      "Epoch [131/150], Loss: 0.32179971029824117, Val Loss: 0.2567211091518402\n",
      "Epoch [132/150], Loss: 0.3210839406287266, Val Loss: 0.25657761096954346\n",
      "Epoch [133/150], Loss: 0.32118186304124735, Val Loss: 0.256436288356781\n",
      "Epoch [134/150], Loss: 0.3206908704744557, Val Loss: 0.256296306848526\n",
      "Epoch [135/150], Loss: 0.32072610045009753, Val Loss: 0.25615784525871277\n",
      "Epoch [136/150], Loss: 0.3200908655269166, Val Loss: 0.25602003931999207\n",
      "Epoch [137/150], Loss: 0.32016509199925397, Val Loss: 0.2558843195438385\n",
      "Epoch [138/150], Loss: 0.3200612895001294, Val Loss: 0.2557509243488312\n",
      "Epoch [139/150], Loss: 0.3195020691570589, Val Loss: 0.255616694688797\n",
      "Epoch [140/150], Loss: 0.3192928807210114, Val Loss: 0.25548410415649414\n",
      "Epoch [141/150], Loss: 0.3188943128275164, Val Loss: 0.2553519010543823\n",
      "Epoch [142/150], Loss: 0.3189954284452281, Val Loss: 0.25522252917289734\n",
      "Epoch [143/150], Loss: 0.31856564915407515, Val Loss: 0.2550938129425049\n",
      "Epoch [144/150], Loss: 0.3185332155454967, Val Loss: 0.2549669146537781\n",
      "Epoch [145/150], Loss: 0.31777052527640837, Val Loss: 0.25483980774879456\n",
      "Epoch [146/150], Loss: 0.3177753964407464, Val Loss: 0.2547146677970886\n",
      "Epoch [147/150], Loss: 0.31748098690631027, Val Loss: 0.2545901834964752\n",
      "Epoch [148/150], Loss: 0.3170366318801702, Val Loss: 0.25446605682373047\n",
      "Epoch [149/150], Loss: 0.3169008429651543, Val Loss: 0.2543428838253021\n",
      "Epoch [150/150], Loss: 0.31684532938367227, Val Loss: 0.2542228400707245\n",
      "MSE Loss: 0.2542228400707245\n",
      "Evaluating with parameters: {'lr': 0.00016193566689591532, 'hidden_size': 60, 'num_layers': 2, 'dropout_rate_1': 0.313748315628618, 'dropout_rate_2': 0.08549764771014451, 'dropout_rate_fc': 0.055014861933887, 'weight_decay': 0.5234193119454468, 'batch_size': 42, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([1, 42])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.2624893104657531, Val Loss: 0.23043808341026306\n",
      "Epoch [2/150], Loss: 0.2587579749778946, Val Loss: 0.23002737760543823\n",
      "Epoch [3/150], Loss: 0.2578749622408097, Val Loss: 0.23010821640491486\n",
      "Epoch [4/150], Loss: 0.2583316520537336, Val Loss: 0.23033036291599274\n",
      "Epoch [5/150], Loss: 0.2590135567901781, Val Loss: 0.23052702844142914\n",
      "Epoch [6/150], Loss: 0.2594773297730301, Val Loss: 0.23061710596084595\n",
      "Epoch [7/150], Loss: 0.2595448973314727, Val Loss: 0.23057201504707336\n",
      "Epoch [8/150], Loss: 0.25912836190320926, Val Loss: 0.23038963973522186\n",
      "Epoch [9/150], Loss: 0.25827099258733616, Val Loss: 0.23008683323860168\n",
      "Epoch [10/150], Loss: 0.25702008770723994, Val Loss: 0.2296917736530304\n",
      "Epoch [11/150], Loss: 0.25541033850886913, Val Loss: 0.22922973334789276\n",
      "Epoch [12/150], Loss: 0.2535555067916299, Val Loss: 0.22873133420944214\n",
      "Epoch [13/150], Loss: 0.25149591541809946, Val Loss: 0.22822287678718567\n",
      "Epoch [14/150], Loss: 0.24928295324867622, Val Loss: 0.227726548910141\n",
      "Epoch [15/150], Loss: 0.24703021253020885, Val Loss: 0.22726428508758545\n",
      "Epoch [16/150], Loss: 0.2447523804451926, Val Loss: 0.22684916853904724\n",
      "Epoch [17/150], Loss: 0.24248635052153528, Val Loss: 0.22649018466472626\n",
      "Epoch [18/150], Loss: 0.24027504551834858, Val Loss: 0.22619278728961945\n",
      "Epoch [19/150], Loss: 0.23814398119081312, Val Loss: 0.2259586900472641\n",
      "Epoch [20/150], Loss: 0.2361079369693165, Val Loss: 0.2257864624261856\n",
      "Epoch [21/150], Loss: 0.23418021530593833, Val Loss: 0.2256728559732437\n",
      "Epoch [22/150], Loss: 0.23236719617314888, Val Loss: 0.22561314702033997\n",
      "Epoch [23/150], Loss: 0.23067230283000953, Val Loss: 0.22560173273086548\n",
      "Epoch [24/150], Loss: 0.2290928063624609, Val Loss: 0.22563284635543823\n",
      "Epoch [25/150], Loss: 0.2276251006973203, Val Loss: 0.22570067644119263\n",
      "Epoch [26/150], Loss: 0.22626427254451736, Val Loss: 0.22579975426197052\n",
      "Epoch [27/150], Loss: 0.2250036440218384, Val Loss: 0.2259250432252884\n",
      "Epoch [28/150], Loss: 0.2238364302114967, Val Loss: 0.2260720431804657\n",
      "Epoch [29/150], Loss: 0.22275591440530135, Val Loss: 0.2262367457151413\n",
      "Epoch [30/150], Loss: 0.2217556038957496, Val Loss: 0.2264155149459839\n",
      "Epoch [31/150], Loss: 0.2208292927111679, Val Loss: 0.22660522162914276\n",
      "Epoch [32/150], Loss: 0.2199713022980307, Val Loss: 0.22680319845676422\n",
      "Epoch [33/150], Loss: 0.21917626620739886, Val Loss: 0.2270069271326065\n",
      "Early stopping at epoch 33\n",
      "MSE Loss: 0.22560173273086548\n",
      "Evaluating with parameters: {'lr': 0.00041113630382443825, 'hidden_size': 178, 'num_layers': 3, 'dropout_rate_1': 0.4273064509965479, 'dropout_rate_2': 0.21400074008852243, 'dropout_rate_fc': 0.2592061729170382, 'weight_decay': 0.22804681852389685, 'batch_size': 21, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([1, 21])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.28153755897086286, Val Loss: 0.22677768766880035\n",
      "Epoch [2/150], Loss: 0.23941119001045597, Val Loss: 0.2258443981409073\n",
      "Epoch [3/150], Loss: 0.2352778013041125, Val Loss: 0.22567778825759888\n",
      "Epoch [4/150], Loss: 0.2326231468923268, Val Loss: 0.22562570869922638\n",
      "Epoch [5/150], Loss: 0.23057110593747543, Val Loss: 0.22565971314907074\n",
      "Epoch [6/150], Loss: 0.22796768973149725, Val Loss: 0.2258041352033615\n",
      "Epoch [7/150], Loss: 0.22562992363015802, Val Loss: 0.2260734736919403\n",
      "Epoch [8/150], Loss: 0.2233927677078998, Val Loss: 0.2264787256717682\n",
      "Epoch [9/150], Loss: 0.22095981038707005, Val Loss: 0.22710192203521729\n",
      "Epoch [10/150], Loss: 0.21850686001783323, Val Loss: 0.2279781848192215\n",
      "Epoch [11/150], Loss: 0.215982329753733, Val Loss: 0.2291620522737503\n",
      "Epoch [12/150], Loss: 0.21350396011398765, Val Loss: 0.23061877489089966\n",
      "Epoch [13/150], Loss: 0.21133091579003876, Val Loss: 0.23221440613269806\n",
      "Epoch [14/150], Loss: 0.20952593816405227, Val Loss: 0.23380136489868164\n",
      "Early stopping at epoch 14\n",
      "MSE Loss: 0.22562570869922638\n",
      "Evaluating with parameters: {'lr': 2.518861054764877e-05, 'hidden_size': 128, 'num_layers': 2, 'dropout_rate_1': 0.16337860571220517, 'dropout_rate_2': 0.522576504945755, 'dropout_rate_fc': 0.5678776026703417, 'weight_decay': 0.8152294339575646, 'batch_size': 71, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([71])) that is different to the input size (torch.Size([1, 71])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([59])) that is different to the input size (torch.Size([1, 59])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.3095720180709447, Val Loss: 0.24973388016223907\n",
      "Epoch [2/150], Loss: 0.30675807563321933, Val Loss: 0.247860386967659\n",
      "Epoch [3/150], Loss: 0.30317050157380954, Val Loss: 0.24623900651931763\n",
      "Epoch [4/150], Loss: 0.29934117799358706, Val Loss: 0.2448509931564331\n",
      "Epoch [5/150], Loss: 0.2966514539771846, Val Loss: 0.2436772882938385\n",
      "Epoch [6/150], Loss: 0.2942447155714035, Val Loss: 0.24264425039291382\n",
      "Epoch [7/150], Loss: 0.29278582829449856, Val Loss: 0.24175697565078735\n",
      "Epoch [8/150], Loss: 0.29074714862342393, Val Loss: 0.24099864065647125\n",
      "Epoch [9/150], Loss: 0.2883304869356964, Val Loss: 0.24034014344215393\n",
      "Epoch [10/150], Loss: 0.2877640033939055, Val Loss: 0.23976607620716095\n",
      "Epoch [11/150], Loss: 0.28602418122547013, Val Loss: 0.23926371335983276\n",
      "Epoch [12/150], Loss: 0.2845903627973582, Val Loss: 0.23882706463336945\n",
      "Epoch [13/150], Loss: 0.2835846660791763, Val Loss: 0.23845155537128448\n",
      "Epoch [14/150], Loss: 0.28291409706164683, Val Loss: 0.2381216436624527\n",
      "Epoch [15/150], Loss: 0.2820068720329021, Val Loss: 0.23783943057060242\n",
      "Epoch [16/150], Loss: 0.2811536061178361, Val Loss: 0.23759007453918457\n",
      "Epoch [17/150], Loss: 0.2807428923169417, Val Loss: 0.2373744547367096\n",
      "Epoch [18/150], Loss: 0.280102149423744, Val Loss: 0.23719266057014465\n",
      "Epoch [19/150], Loss: 0.2798287620767951, Val Loss: 0.2370329052209854\n",
      "Epoch [20/150], Loss: 0.27951453333454473, Val Loss: 0.2368980199098587\n",
      "Epoch [21/150], Loss: 0.2789586998655328, Val Loss: 0.23678529262542725\n",
      "Epoch [22/150], Loss: 0.27908745974834476, Val Loss: 0.23669467866420746\n",
      "Epoch [23/150], Loss: 0.2786637797951698, Val Loss: 0.23661260306835175\n",
      "Epoch [24/150], Loss: 0.2784502206237188, Val Loss: 0.23654548823833466\n",
      "Epoch [25/150], Loss: 0.2784534130112401, Val Loss: 0.23649559915065765\n",
      "Epoch [26/150], Loss: 0.27803472018401537, Val Loss: 0.23645009100437164\n",
      "Epoch [27/150], Loss: 0.27794124046340585, Val Loss: 0.23641405999660492\n",
      "Epoch [28/150], Loss: 0.277831261990858, Val Loss: 0.23638670146465302\n",
      "Epoch [29/150], Loss: 0.2776090777346066, Val Loss: 0.23636139929294586\n",
      "Epoch [30/150], Loss: 0.2776823370983558, Val Loss: 0.23634573817253113\n",
      "Epoch [31/150], Loss: 0.2776606068946421, Val Loss: 0.23633639514446259\n",
      "Epoch [32/150], Loss: 0.2776737872378102, Val Loss: 0.23633262515068054\n",
      "Epoch [33/150], Loss: 0.27761143998109866, Val Loss: 0.2363288402557373\n",
      "Epoch [34/150], Loss: 0.2776344190617757, Val Loss: 0.23632819950580597\n",
      "Epoch [35/150], Loss: 0.27774568262643046, Val Loss: 0.2363305538892746\n",
      "Epoch [36/150], Loss: 0.2777403845052634, Val Loss: 0.2363346368074417\n",
      "Epoch [37/150], Loss: 0.27780188479061635, Val Loss: 0.2363385111093521\n",
      "Epoch [38/150], Loss: 0.27763368070923855, Val Loss: 0.23633919656276703\n",
      "Epoch [39/150], Loss: 0.2776731745206884, Val Loss: 0.23633915185928345\n",
      "Epoch [40/150], Loss: 0.2776194312476686, Val Loss: 0.23633678257465363\n",
      "Epoch [41/150], Loss: 0.2776888823934964, Val Loss: 0.23633545637130737\n",
      "Epoch [42/150], Loss: 0.27770830053570017, Val Loss: 0.2363317459821701\n",
      "Epoch [43/150], Loss: 0.27771892024736317, Val Loss: 0.23632477223873138\n",
      "Epoch [44/150], Loss: 0.2776353931852749, Val Loss: 0.2363133579492569\n",
      "Epoch [45/150], Loss: 0.27761156662766423, Val Loss: 0.2362992763519287\n",
      "Epoch [46/150], Loss: 0.2775453010307891, Val Loss: 0.23628121614456177\n",
      "Epoch [47/150], Loss: 0.2774676933751575, Val Loss: 0.23625946044921875\n",
      "Epoch [48/150], Loss: 0.277399749056037, Val Loss: 0.23623284697532654\n",
      "Epoch [49/150], Loss: 0.2772710531816951, Val Loss: 0.23620104789733887\n",
      "Epoch [50/150], Loss: 0.2772021323043321, Val Loss: 0.236164852976799\n",
      "Epoch [51/150], Loss: 0.2770853826643101, Val Loss: 0.23612409830093384\n",
      "Epoch [52/150], Loss: 0.2769700961985758, Val Loss: 0.23607926070690155\n",
      "Epoch [53/150], Loss: 0.27688665451215844, Val Loss: 0.23603010177612305\n",
      "Epoch [54/150], Loss: 0.27668809003329703, Val Loss: 0.23597490787506104\n",
      "Epoch [55/150], Loss: 0.2765712325860347, Val Loss: 0.2359156459569931\n",
      "Epoch [56/150], Loss: 0.2764048486016691, Val Loss: 0.2358519434928894\n",
      "Epoch [57/150], Loss: 0.2762192546123905, Val Loss: 0.23578326404094696\n",
      "Epoch [58/150], Loss: 0.2760188582619386, Val Loss: 0.23570996522903442\n",
      "Epoch [59/150], Loss: 0.2758201635309628, Val Loss: 0.23563241958618164\n",
      "Epoch [60/150], Loss: 0.2756254361543272, Val Loss: 0.23555128276348114\n",
      "Epoch [61/150], Loss: 0.275363442953676, Val Loss: 0.23546472191810608\n",
      "Epoch [62/150], Loss: 0.27513633321172426, Val Loss: 0.23537439107894897\n",
      "Epoch [63/150], Loss: 0.27484129817624176, Val Loss: 0.23527930676937103\n",
      "Epoch [64/150], Loss: 0.27461770003927605, Val Loss: 0.23518164455890656\n",
      "Epoch [65/150], Loss: 0.2743517266453377, Val Loss: 0.23508045077323914\n",
      "Epoch [66/150], Loss: 0.27404475579304355, Val Loss: 0.23497560620307922\n",
      "Epoch [67/150], Loss: 0.2737630617937871, Val Loss: 0.2348679006099701\n",
      "Epoch [68/150], Loss: 0.27344426760183915, Val Loss: 0.23475703597068787\n",
      "Epoch [69/150], Loss: 0.27313763714794603, Val Loss: 0.23464380204677582\n",
      "Epoch [70/150], Loss: 0.2728323168520417, Val Loss: 0.23452872037887573\n",
      "Epoch [71/150], Loss: 0.272512787380921, Val Loss: 0.23441167175769806\n",
      "Epoch [72/150], Loss: 0.2721739719488791, Val Loss: 0.23429250717163086\n",
      "Epoch [73/150], Loss: 0.27183474794562373, Val Loss: 0.23417165875434875\n",
      "Epoch [74/150], Loss: 0.2714831972494721, Val Loss: 0.23404935002326965\n",
      "Epoch [75/150], Loss: 0.27113387105055153, Val Loss: 0.233925923705101\n",
      "Epoch [76/150], Loss: 0.27077457733851457, Val Loss: 0.23380140960216522\n",
      "Epoch [77/150], Loss: 0.27041427016790426, Val Loss: 0.2336762398481369\n",
      "Epoch [78/150], Loss: 0.27004485857406896, Val Loss: 0.23355038464069366\n",
      "Epoch [79/150], Loss: 0.2696840924410416, Val Loss: 0.2334243804216385\n",
      "Epoch [80/150], Loss: 0.2693198730370828, Val Loss: 0.2332984209060669\n",
      "Epoch [81/150], Loss: 0.2689441811293364, Val Loss: 0.23317238688468933\n",
      "Epoch [82/150], Loss: 0.2685704386393939, Val Loss: 0.2330465465784073\n",
      "Epoch [83/150], Loss: 0.26819829384663274, Val Loss: 0.23292110860347748\n",
      "Epoch [84/150], Loss: 0.2678205640055239, Val Loss: 0.2327961027622223\n",
      "Epoch [85/150], Loss: 0.2674488608858415, Val Loss: 0.2326718270778656\n",
      "Epoch [86/150], Loss: 0.26706855474039914, Val Loss: 0.23254813253879547\n",
      "Epoch [87/150], Loss: 0.2666968491061458, Val Loss: 0.23242546617984772\n",
      "Epoch [88/150], Loss: 0.26632209178725524, Val Loss: 0.2323037087917328\n",
      "Epoch [89/150], Loss: 0.2659468955559922, Val Loss: 0.23218294978141785\n",
      "Epoch [90/150], Loss: 0.2655754144103932, Val Loss: 0.2320633828639984\n",
      "Epoch [91/150], Loss: 0.2652034872477608, Val Loss: 0.23194491863250732\n",
      "Epoch [92/150], Loss: 0.2648329832125455, Val Loss: 0.2318277209997177\n",
      "Epoch [93/150], Loss: 0.26446554638844516, Val Loss: 0.23171181976795197\n",
      "Epoch [94/150], Loss: 0.26409795181825757, Val Loss: 0.2315971851348877\n",
      "Epoch [95/150], Loss: 0.26373299688899093, Val Loss: 0.23148395121097565\n",
      "Epoch [96/150], Loss: 0.2633701720341508, Val Loss: 0.23137204349040985\n",
      "Epoch [97/150], Loss: 0.2630098524769502, Val Loss: 0.23126158118247986\n",
      "Epoch [98/150], Loss: 0.2626511667788561, Val Loss: 0.23115253448486328\n",
      "Epoch [99/150], Loss: 0.26229510003301715, Val Loss: 0.2310449331998825\n",
      "Epoch [100/150], Loss: 0.26194126176913934, Val Loss: 0.23093876242637634\n",
      "Epoch [101/150], Loss: 0.26158988609510875, Val Loss: 0.23083406686782837\n",
      "Epoch [102/150], Loss: 0.26124060549773276, Val Loss: 0.23073077201843262\n",
      "Epoch [103/150], Loss: 0.26089416278659233, Val Loss: 0.23062896728515625\n",
      "Epoch [104/150], Loss: 0.26054984095639416, Val Loss: 0.2305285930633545\n",
      "Epoch [105/150], Loss: 0.2602082627958485, Val Loss: 0.23042966425418854\n",
      "Epoch [106/150], Loss: 0.25986911903933757, Val Loss: 0.23033221065998077\n",
      "Epoch [107/150], Loss: 0.2595324050036392, Val Loss: 0.23023612797260284\n",
      "Epoch [108/150], Loss: 0.2591982504685542, Val Loss: 0.23014147579669952\n",
      "Epoch [109/150], Loss: 0.2588666020360376, Val Loss: 0.23004823923110962\n",
      "Epoch [110/150], Loss: 0.25853745437759373, Val Loss: 0.22995641827583313\n",
      "Epoch [111/150], Loss: 0.25821088403463366, Val Loss: 0.22986595332622528\n",
      "Epoch [112/150], Loss: 0.2578867833196585, Val Loss: 0.22977690398693085\n",
      "Epoch [113/150], Loss: 0.2575652082982872, Val Loss: 0.22968922555446625\n",
      "Epoch [114/150], Loss: 0.25724604846244414, Val Loss: 0.22960282862186432\n",
      "Epoch [115/150], Loss: 0.25692938435822726, Val Loss: 0.22951777279376984\n",
      "Epoch [116/150], Loss: 0.25661516898045583, Val Loss: 0.22943410277366638\n",
      "Epoch [117/150], Loss: 0.256303410252024, Val Loss: 0.2293516993522644\n",
      "Epoch [118/150], Loss: 0.2559940780845604, Val Loss: 0.2292705923318863\n",
      "Epoch [119/150], Loss: 0.25568716750214143, Val Loss: 0.22919079661369324\n",
      "Epoch [120/150], Loss: 0.2553826637433044, Val Loss: 0.22911223769187927\n",
      "Epoch [121/150], Loss: 0.25508051834601375, Val Loss: 0.22903496026992798\n",
      "Epoch [122/150], Loss: 0.2547807415681226, Val Loss: 0.228958860039711\n",
      "Epoch [123/150], Loss: 0.25448332049085626, Val Loss: 0.2288840413093567\n",
      "Epoch [124/150], Loss: 0.2541882454151554, Val Loss: 0.22881045937538147\n",
      "Epoch [125/150], Loss: 0.25389550420056495, Val Loss: 0.22873800992965698\n",
      "Epoch [126/150], Loss: 0.2536050566671682, Val Loss: 0.22866681218147278\n",
      "Epoch [127/150], Loss: 0.2533168797248176, Val Loss: 0.2285967618227005\n",
      "Epoch [128/150], Loss: 0.2530309750898076, Val Loss: 0.22852785885334015\n",
      "Epoch [129/150], Loss: 0.2527473171540935, Val Loss: 0.22846010327339172\n",
      "Epoch [130/150], Loss: 0.2524659108270758, Val Loss: 0.22839349508285522\n",
      "Epoch [131/150], Loss: 0.2521867416300146, Val Loss: 0.22832798957824707\n",
      "Epoch [132/150], Loss: 0.2519097419621955, Val Loss: 0.22826360166072845\n",
      "Epoch [133/150], Loss: 0.2516349538595283, Val Loss: 0.228200301527977\n",
      "Epoch [134/150], Loss: 0.25136231980619156, Val Loss: 0.2281380593776703\n",
      "Epoch [135/150], Loss: 0.25109184584580363, Val Loss: 0.22807694971561432\n",
      "Epoch [136/150], Loss: 0.25082350696237493, Val Loss: 0.22801685333251953\n",
      "Epoch [137/150], Loss: 0.25055728253708887, Val Loss: 0.22795778512954712\n",
      "Epoch [138/150], Loss: 0.25029319537071776, Val Loss: 0.22789980471134186\n",
      "Epoch [139/150], Loss: 0.2500312180790518, Val Loss: 0.227842777967453\n",
      "Epoch [140/150], Loss: 0.2497713092514979, Val Loss: 0.22778679430484772\n",
      "Epoch [141/150], Loss: 0.2495134811914925, Val Loss: 0.22773180902004242\n",
      "Epoch [142/150], Loss: 0.24925769978269402, Val Loss: 0.2276778221130371\n",
      "Epoch [143/150], Loss: 0.2490039394802547, Val Loss: 0.22762475907802582\n",
      "Epoch [144/150], Loss: 0.24875219981518706, Val Loss: 0.22757267951965332\n",
      "Epoch [145/150], Loss: 0.2485024723823049, Val Loss: 0.22752158343791962\n",
      "Epoch [146/150], Loss: 0.24825473478995264, Val Loss: 0.22747138142585754\n",
      "Epoch [147/150], Loss: 0.24800897477128145, Val Loss: 0.2274221032857895\n",
      "Epoch [148/150], Loss: 0.24776518692129423, Val Loss: 0.22737377882003784\n",
      "Epoch [149/150], Loss: 0.24752335964169886, Val Loss: 0.22732631862163544\n",
      "Epoch [150/150], Loss: 0.2472834398204993, Val Loss: 0.22727975249290466\n",
      "MSE Loss: 0.22727975249290466\n",
      "Evaluating with parameters: {'lr': 6.598015455820287e-06, 'hidden_size': 193, 'num_layers': 2, 'dropout_rate_1': 0.07416743412613869, 'dropout_rate_2': 0.3724729496054351, 'dropout_rate_fc': 0.5220011093653738, 'weight_decay': 0.008367310053257271, 'batch_size': 16, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([1, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([1, 14])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.38507676934410306, Val Loss: 0.2714443504810333\n",
      "Epoch [2/150], Loss: 0.3422239865568812, Val Loss: 0.24557064473628998\n",
      "Epoch [3/150], Loss: 0.3013233460944086, Val Loss: 0.22625800967216492\n",
      "Epoch [4/150], Loss: 0.266291497746152, Val Loss: 0.21911832690238953\n",
      "Epoch [5/150], Loss: 0.2404950825542572, Val Loss: 0.22475329041481018\n",
      "Epoch [6/150], Loss: 0.22556188371663372, Val Loss: 0.23343485593795776\n",
      "Epoch [7/150], Loss: 0.21801282383291232, Val Loss: 0.23818837106227875\n",
      "Epoch [8/150], Loss: 0.21176300298632875, Val Loss: 0.23963652551174164\n",
      "Epoch [9/150], Loss: 0.2075324692297727, Val Loss: 0.23943443596363068\n",
      "Epoch [10/150], Loss: 0.20363139167067504, Val Loss: 0.23770296573638916\n",
      "Epoch [11/150], Loss: 0.20034019196736477, Val Loss: 0.23509648442268372\n",
      "Epoch [12/150], Loss: 0.19596828944500416, Val Loss: 0.2330223023891449\n",
      "Epoch [13/150], Loss: 0.19270318128138542, Val Loss: 0.23108166456222534\n",
      "Epoch [14/150], Loss: 0.19123371487273083, Val Loss: 0.22917869687080383\n",
      "Early stopping at epoch 14\n",
      "MSE Loss: 0.21911832690238953\n",
      "Evaluating with parameters: {'lr': 0.0006061876863976136, 'hidden_size': 113, 'num_layers': 3, 'dropout_rate_1': 0.5165149170905351, 'dropout_rate_2': 0.33198874192312355, 'dropout_rate_fc': 0.1299142607487738, 'weight_decay': 0.9100294686902491, 'batch_size': 66, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([66])) that is different to the input size (torch.Size([1, 66])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.36730089658675225, Val Loss: 0.274469792842865\n",
      "Epoch [2/150], Loss: 0.35034867334424663, Val Loss: 0.27080240845680237\n",
      "Epoch [3/150], Loss: 0.34155541511350557, Val Loss: 0.2636260390281677\n",
      "Epoch [4/150], Loss: 0.32634552430949715, Val Loss: 0.25486740469932556\n",
      "Epoch [5/150], Loss: 0.3096432223621952, Val Loss: 0.2470821738243103\n",
      "Epoch [6/150], Loss: 0.2944989145096195, Val Loss: 0.24091894924640656\n",
      "Epoch [7/150], Loss: 0.2816092611633633, Val Loss: 0.2362452745437622\n",
      "Epoch [8/150], Loss: 0.2708254796330278, Val Loss: 0.23278281092643738\n",
      "Epoch [9/150], Loss: 0.2618431039522157, Val Loss: 0.23027220368385315\n",
      "Epoch [10/150], Loss: 0.25437228358963404, Val Loss: 0.2284969687461853\n",
      "Epoch [11/150], Loss: 0.2481627877568826, Val Loss: 0.22728128731250763\n",
      "Epoch [12/150], Loss: 0.24300279229952904, Val Loss: 0.22648431360721588\n",
      "Epoch [13/150], Loss: 0.23871409929798623, Val Loss: 0.22599506378173828\n",
      "Epoch [14/150], Loss: 0.2351478122383062, Val Loss: 0.22572720050811768\n",
      "Epoch [15/150], Loss: 0.23217998461577258, Val Loss: 0.22561465203762054\n",
      "Epoch [16/150], Loss: 0.22970779733977428, Val Loss: 0.22560769319534302\n",
      "Epoch [17/150], Loss: 0.22764629593400873, Val Loss: 0.22566938400268555\n",
      "Epoch [18/150], Loss: 0.22592527782667035, Val Loss: 0.22577300667762756\n",
      "Epoch [19/150], Loss: 0.22448687220764296, Val Loss: 0.22589938342571259\n",
      "Epoch [20/150], Loss: 0.22328336016691633, Val Loss: 0.22603532671928406\n",
      "Epoch [21/150], Loss: 0.22227537268037467, Val Loss: 0.22617189586162567\n",
      "Epoch [22/150], Loss: 0.22143033181782812, Val Loss: 0.2263033390045166\n",
      "Epoch [23/150], Loss: 0.22072131591113775, Val Loss: 0.22642622888088226\n",
      "Epoch [24/150], Loss: 0.2201260097024619, Val Loss: 0.22653871774673462\n",
      "Epoch [25/150], Loss: 0.21962589076658906, Val Loss: 0.2266400009393692\n",
      "Epoch [26/150], Loss: 0.21920562154111012, Val Loss: 0.22673015296459198\n",
      "Early stopping at epoch 26\n",
      "MSE Loss: 0.22560769319534302\n",
      "Evaluating with parameters: {'lr': 4.7371837924837566e-05, 'hidden_size': 228, 'num_layers': 2, 'dropout_rate_1': 0.2245418196544051, 'dropout_rate_2': 0.11055430443957447, 'dropout_rate_fc': 0.27710951771587133, 'weight_decay': 0.341436113350369, 'batch_size': 91, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([91])) that is different to the input size (torch.Size([1, 91])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([44])) that is different to the input size (torch.Size([1, 44])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.3321561343967915, Val Loss: 0.25825268030166626\n",
      "Epoch [2/150], Loss: 0.319541310756044, Val Loss: 0.2525310814380646\n",
      "Epoch [3/150], Loss: 0.3096259696239775, Val Loss: 0.2482045590877533\n",
      "Epoch [4/150], Loss: 0.30162279250269586, Val Loss: 0.244896799325943\n",
      "Epoch [5/150], Loss: 0.29492569765584037, Val Loss: 0.24228373169898987\n",
      "Epoch [6/150], Loss: 0.28947007620537824, Val Loss: 0.24020060896873474\n",
      "Epoch [7/150], Loss: 0.28502742527899416, Val Loss: 0.23851251602172852\n",
      "Epoch [8/150], Loss: 0.28090474884110417, Val Loss: 0.23710711300373077\n",
      "Epoch [9/150], Loss: 0.27784978102215313, Val Loss: 0.2359539419412613\n",
      "Epoch [10/150], Loss: 0.2749295857819644, Val Loss: 0.2349773496389389\n",
      "Epoch [11/150], Loss: 0.2721877534535121, Val Loss: 0.23413772881031036\n",
      "Epoch [12/150], Loss: 0.27015708521516485, Val Loss: 0.2334299385547638\n",
      "Epoch [13/150], Loss: 0.2683527613736012, Val Loss: 0.23282037675380707\n",
      "Epoch [14/150], Loss: 0.26630964407019997, Val Loss: 0.23228439688682556\n",
      "Epoch [15/150], Loss: 0.2649380800822242, Val Loss: 0.2318260222673416\n",
      "Epoch [16/150], Loss: 0.2634276421232657, Val Loss: 0.23141661286354065\n",
      "Epoch [17/150], Loss: 0.26234868032519115, Val Loss: 0.23106391727924347\n",
      "Epoch [18/150], Loss: 0.2612874208526178, Val Loss: 0.23075206577777863\n",
      "Epoch [19/150], Loss: 0.26013537889177146, Val Loss: 0.23047703504562378\n",
      "Epoch [20/150], Loss: 0.2595195955935527, Val Loss: 0.23023435473442078\n",
      "Epoch [21/150], Loss: 0.2583619185384702, Val Loss: 0.2300129532814026\n",
      "Epoch [22/150], Loss: 0.25779330247843807, Val Loss: 0.22981995344161987\n",
      "Epoch [23/150], Loss: 0.25715111463584683, Val Loss: 0.22964678704738617\n",
      "Epoch [24/150], Loss: 0.2567064222879708, Val Loss: 0.2294953316450119\n",
      "Epoch [25/150], Loss: 0.25641407326541166, Val Loss: 0.22936242818832397\n",
      "Epoch [26/150], Loss: 0.2553217358717864, Val Loss: 0.2292328029870987\n",
      "Epoch [27/150], Loss: 0.255067435880615, Val Loss: 0.22911743819713593\n",
      "Epoch [28/150], Loss: 0.25441457321867345, Val Loss: 0.22900860011577606\n",
      "Epoch [29/150], Loss: 0.25400518175553194, Val Loss: 0.22890929877758026\n",
      "Epoch [30/150], Loss: 0.25354490850798106, Val Loss: 0.22881793975830078\n",
      "Epoch [31/150], Loss: 0.25348886042325336, Val Loss: 0.22874000668525696\n",
      "Epoch [32/150], Loss: 0.2530218551358716, Val Loss: 0.22866299748420715\n",
      "Epoch [33/150], Loss: 0.2525980342179537, Val Loss: 0.22859250009059906\n",
      "Epoch [34/150], Loss: 0.2526668877933513, Val Loss: 0.2285318225622177\n",
      "Epoch [35/150], Loss: 0.2524463643489236, Val Loss: 0.22847437858581543\n",
      "Epoch [36/150], Loss: 0.25194735592231154, Val Loss: 0.22841913998126984\n",
      "Epoch [37/150], Loss: 0.25193477433852174, Val Loss: 0.22837123274803162\n",
      "Epoch [38/150], Loss: 0.25172274632548747, Val Loss: 0.22832855582237244\n",
      "Epoch [39/150], Loss: 0.2513494835235178, Val Loss: 0.22828376293182373\n",
      "Epoch [40/150], Loss: 0.2511919846080921, Val Loss: 0.2282429039478302\n",
      "Epoch [41/150], Loss: 0.25105283560062, Val Loss: 0.2282036542892456\n",
      "Epoch [42/150], Loss: 0.2507751305841587, Val Loss: 0.2281668782234192\n",
      "Epoch [43/150], Loss: 0.2508301999843256, Val Loss: 0.2281365841627121\n",
      "Epoch [44/150], Loss: 0.2504095984453505, Val Loss: 0.22810322046279907\n",
      "Epoch [45/150], Loss: 0.2504906347868117, Val Loss: 0.22807557880878448\n",
      "Epoch [46/150], Loss: 0.25011523167856714, Val Loss: 0.22804392874240875\n",
      "Epoch [47/150], Loss: 0.25025635767220095, Val Loss: 0.22801794111728668\n",
      "Epoch [48/150], Loss: 0.24994469836184924, Val Loss: 0.2279929369688034\n",
      "Epoch [49/150], Loss: 0.24999650327319448, Val Loss: 0.22797147929668427\n",
      "Epoch [50/150], Loss: 0.24966073910790412, Val Loss: 0.22794649004936218\n",
      "Epoch [51/150], Loss: 0.2497625477527353, Val Loss: 0.22792702913284302\n",
      "Epoch [52/150], Loss: 0.24970294874669477, Val Loss: 0.22790896892547607\n",
      "Epoch [53/150], Loss: 0.24955344062975862, Val Loss: 0.22788982093334198\n",
      "Epoch [54/150], Loss: 0.24956142763522537, Val Loss: 0.22787386178970337\n",
      "Epoch [55/150], Loss: 0.24943922973999924, Val Loss: 0.2278570532798767\n",
      "Epoch [56/150], Loss: 0.24922568259591407, Val Loss: 0.22783926129341125\n",
      "Epoch [57/150], Loss: 0.24900348807515746, Val Loss: 0.22782015800476074\n",
      "Epoch [58/150], Loss: 0.2489931347695264, Val Loss: 0.2278032749891281\n",
      "Epoch [59/150], Loss: 0.24891620226712388, Val Loss: 0.22778581082820892\n",
      "Epoch [60/150], Loss: 0.24890144585885784, Val Loss: 0.2277725636959076\n",
      "Epoch [61/150], Loss: 0.24879864193499088, Val Loss: 0.22775952517986298\n",
      "Epoch [62/150], Loss: 0.24871942411938852, Val Loss: 0.2277444303035736\n",
      "Epoch [63/150], Loss: 0.24866144017909061, Val Loss: 0.22773022949695587\n",
      "Epoch [64/150], Loss: 0.24880142235620456, Val Loss: 0.2277209311723709\n",
      "Epoch [65/150], Loss: 0.24859539055350152, Val Loss: 0.22770710289478302\n",
      "Epoch [66/150], Loss: 0.2484645235690881, Val Loss: 0.22769388556480408\n",
      "Epoch [67/150], Loss: 0.24845391859893096, Val Loss: 0.22768233716487885\n",
      "Epoch [68/150], Loss: 0.2482884240421382, Val Loss: 0.2276676744222641\n",
      "Epoch [69/150], Loss: 0.24803901164453815, Val Loss: 0.2276509404182434\n",
      "Epoch [70/150], Loss: 0.2481719874438237, Val Loss: 0.22763755917549133\n",
      "Epoch [71/150], Loss: 0.24824123972688208, Val Loss: 0.22762702405452728\n",
      "Epoch [72/150], Loss: 0.24802218904210763, Val Loss: 0.2276138961315155\n",
      "Epoch [73/150], Loss: 0.24816075109199368, Val Loss: 0.22760455310344696\n",
      "Epoch [74/150], Loss: 0.24815954813225702, Val Loss: 0.22759568691253662\n",
      "Epoch [75/150], Loss: 0.2478251489471983, Val Loss: 0.22758106887340546\n",
      "Epoch [76/150], Loss: 0.2477680100297386, Val Loss: 0.22756803035736084\n",
      "Epoch [77/150], Loss: 0.24777239392172884, Val Loss: 0.22755633294582367\n",
      "Epoch [78/150], Loss: 0.24770798966373234, Val Loss: 0.22754496335983276\n",
      "Epoch [79/150], Loss: 0.2476590713570741, Val Loss: 0.22753295302391052\n",
      "Epoch [80/150], Loss: 0.24764072834090753, Val Loss: 0.22752203047275543\n",
      "Epoch [81/150], Loss: 0.247423848323524, Val Loss: 0.22750896215438843\n",
      "Epoch [82/150], Loss: 0.24744923769242383, Val Loss: 0.22749771177768707\n",
      "Epoch [83/150], Loss: 0.2474903991445899, Val Loss: 0.22748717665672302\n",
      "Epoch [84/150], Loss: 0.24742168423465707, Val Loss: 0.22747573256492615\n",
      "Epoch [85/150], Loss: 0.2472051753577861, Val Loss: 0.22746221721172333\n",
      "Epoch [86/150], Loss: 0.24740572007034312, Val Loss: 0.22745381295681\n",
      "Epoch [87/150], Loss: 0.2470375462985513, Val Loss: 0.22743819653987885\n",
      "Epoch [88/150], Loss: 0.24702234339307655, Val Loss: 0.22742387652397156\n",
      "Epoch [89/150], Loss: 0.24705670489506287, Val Loss: 0.22741292417049408\n",
      "Epoch [90/150], Loss: 0.24705340501598336, Val Loss: 0.2274012565612793\n",
      "Epoch [91/150], Loss: 0.24684514760209078, Val Loss: 0.22738564014434814\n",
      "Epoch [92/150], Loss: 0.24684763791208916, Val Loss: 0.22737254202365875\n",
      "Epoch [93/150], Loss: 0.24680996127849952, Val Loss: 0.22736018896102905\n",
      "Epoch [94/150], Loss: 0.24679592815651136, Val Loss: 0.2273470014333725\n",
      "Epoch [95/150], Loss: 0.24659947896460918, Val Loss: 0.2273332178592682\n",
      "Epoch [96/150], Loss: 0.2465093791908161, Val Loss: 0.22731909155845642\n",
      "Epoch [97/150], Loss: 0.24649297794411806, Val Loss: 0.22730550169944763\n",
      "Epoch [98/150], Loss: 0.24649708585982974, Val Loss: 0.2272932231426239\n",
      "Epoch [99/150], Loss: 0.2462667153343897, Val Loss: 0.22727788984775543\n",
      "Epoch [100/150], Loss: 0.24630517813123085, Val Loss: 0.2272639274597168\n",
      "Epoch [101/150], Loss: 0.24604387120343746, Val Loss: 0.22724688053131104\n",
      "Epoch [102/150], Loss: 0.24602105356752874, Val Loss: 0.22723087668418884\n",
      "Epoch [103/150], Loss: 0.24596266695298255, Val Loss: 0.22721576690673828\n",
      "Epoch [104/150], Loss: 0.2460058653388511, Val Loss: 0.2272016406059265\n",
      "Epoch [105/150], Loss: 0.24580837613902987, Val Loss: 0.2271859347820282\n",
      "Epoch [106/150], Loss: 0.24573617431081154, Val Loss: 0.22716891765594482\n",
      "Epoch [107/150], Loss: 0.24558661278252575, Val Loss: 0.22715212404727936\n",
      "Epoch [108/150], Loss: 0.2455434528983791, Val Loss: 0.2271355390548706\n",
      "Epoch [109/150], Loss: 0.24550832038406623, Val Loss: 0.22712062299251556\n",
      "Epoch [110/150], Loss: 0.24530371125279501, Val Loss: 0.22710222005844116\n",
      "Epoch [111/150], Loss: 0.24530886637724258, Val Loss: 0.22708594799041748\n",
      "Epoch [112/150], Loss: 0.2451561244881966, Val Loss: 0.22706888616085052\n",
      "Epoch [113/150], Loss: 0.24507796276699412, Val Loss: 0.22705207765102386\n",
      "Epoch [114/150], Loss: 0.24501873903738505, Val Loss: 0.2270340919494629\n",
      "Epoch [115/150], Loss: 0.24488058637573637, Val Loss: 0.22701676189899445\n",
      "Epoch [116/150], Loss: 0.24466268237764863, Val Loss: 0.22699742019176483\n",
      "Epoch [117/150], Loss: 0.24462774923833255, Val Loss: 0.22697876393795013\n",
      "Epoch [118/150], Loss: 0.24459455180066553, Val Loss: 0.22696183621883392\n",
      "Epoch [119/150], Loss: 0.2445091408169405, Val Loss: 0.22694431245326996\n",
      "Epoch [120/150], Loss: 0.24444179432043298, Val Loss: 0.22692683339118958\n",
      "Epoch [121/150], Loss: 0.24433097351257774, Val Loss: 0.22690895199775696\n",
      "Epoch [122/150], Loss: 0.24422003276307475, Val Loss: 0.22689110040664673\n",
      "Epoch [123/150], Loss: 0.24407639355314048, Val Loss: 0.22687235474586487\n",
      "Epoch [124/150], Loss: 0.24384605639868162, Val Loss: 0.2268521636724472\n",
      "Epoch [125/150], Loss: 0.24374561591751195, Val Loss: 0.22683194279670715\n",
      "Epoch [126/150], Loss: 0.24373040957037698, Val Loss: 0.22681310772895813\n",
      "Epoch [127/150], Loss: 0.24342677589844575, Val Loss: 0.22679100930690765\n",
      "Epoch [128/150], Loss: 0.2434379542796788, Val Loss: 0.22677084803581238\n",
      "Epoch [129/150], Loss: 0.2432270319539715, Val Loss: 0.22674980759620667\n",
      "Epoch [130/150], Loss: 0.24314198990945113, Val Loss: 0.22672896087169647\n",
      "Epoch [131/150], Loss: 0.243054637177424, Val Loss: 0.22670882940292358\n",
      "Epoch [132/150], Loss: 0.24281811862337319, Val Loss: 0.22668756544589996\n",
      "Epoch [133/150], Loss: 0.2427871430716054, Val Loss: 0.22666700184345245\n",
      "Epoch [134/150], Loss: 0.2425667479634285, Val Loss: 0.22664521634578705\n",
      "Epoch [135/150], Loss: 0.24242964190515606, Val Loss: 0.22662334144115448\n",
      "Epoch [136/150], Loss: 0.24240656997161833, Val Loss: 0.2266029566526413\n",
      "Epoch [137/150], Loss: 0.24237112630666657, Val Loss: 0.22658385336399078\n",
      "Epoch [138/150], Loss: 0.24209302613850345, Val Loss: 0.22656254470348358\n",
      "Epoch [139/150], Loss: 0.2420173982750963, Val Loss: 0.22654196619987488\n",
      "Epoch [140/150], Loss: 0.2418433662419292, Val Loss: 0.22652114927768707\n",
      "Epoch [141/150], Loss: 0.241748136815361, Val Loss: 0.22650040686130524\n",
      "Epoch [142/150], Loss: 0.24152297236194664, Val Loss: 0.22647880017757416\n",
      "Epoch [143/150], Loss: 0.24129223781214518, Val Loss: 0.22645625472068787\n",
      "Epoch [144/150], Loss: 0.24124694568189708, Val Loss: 0.22643500566482544\n",
      "Epoch [145/150], Loss: 0.2411993604322726, Val Loss: 0.22641482949256897\n",
      "Epoch [146/150], Loss: 0.24100353704257446, Val Loss: 0.22639408707618713\n",
      "Epoch [147/150], Loss: 0.2408554074015807, Val Loss: 0.22637313604354858\n",
      "Epoch [148/150], Loss: 0.24070432848212395, Val Loss: 0.22635239362716675\n",
      "Epoch [149/150], Loss: 0.24054119448939507, Val Loss: 0.22633139789104462\n",
      "Epoch [150/150], Loss: 0.24031824297694998, Val Loss: 0.22630952298641205\n",
      "MSE Loss: 0.22630952298641205\n",
      "Evaluating with parameters: {'lr': 2.8179460905235268e-06, 'hidden_size': 78, 'num_layers': 3, 'dropout_rate_1': 0.6723612444475293, 'dropout_rate_2': 0.5938740606419741, 'dropout_rate_fc': 0.667721783183515, 'weight_decay': 0.7411456675669188, 'batch_size': 47, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([47])) that is different to the input size (torch.Size([1, 47])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([1, 23])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.31032369772130447, Val Loss: 0.2499018758535385\n",
      "Epoch [2/150], Loss: 0.31015921205299785, Val Loss: 0.24962212145328522\n",
      "Epoch [3/150], Loss: 0.31043578951865575, Val Loss: 0.24934610724449158\n",
      "Epoch [4/150], Loss: 0.30977088152782395, Val Loss: 0.24907700717449188\n",
      "Epoch [5/150], Loss: 0.30855827686763454, Val Loss: 0.2488100379705429\n",
      "Epoch [6/150], Loss: 0.30812560330707367, Val Loss: 0.24854421615600586\n",
      "Epoch [7/150], Loss: 0.30808839118459597, Val Loss: 0.24828483164310455\n",
      "Epoch [8/150], Loss: 0.3070463494766714, Val Loss: 0.24802690744400024\n",
      "Epoch [9/150], Loss: 0.3066381316342092, Val Loss: 0.2477826029062271\n",
      "Epoch [10/150], Loss: 0.30620231886201027, Val Loss: 0.24753136932849884\n",
      "Epoch [11/150], Loss: 0.3047847171173484, Val Loss: 0.24729560315608978\n",
      "Epoch [12/150], Loss: 0.3050206784677323, Val Loss: 0.24705635011196136\n",
      "Epoch [13/150], Loss: 0.3048602563047887, Val Loss: 0.2468247413635254\n",
      "Epoch [14/150], Loss: 0.3031202740835483, Val Loss: 0.2465994507074356\n",
      "Epoch [15/150], Loss: 0.30257652048021555, Val Loss: 0.24637837707996368\n",
      "Epoch [16/150], Loss: 0.30126420082963723, Val Loss: 0.24615897238254547\n",
      "Epoch [17/150], Loss: 0.3036544597048734, Val Loss: 0.2459450215101242\n",
      "Epoch [18/150], Loss: 0.30306515911427095, Val Loss: 0.24573661386966705\n",
      "Epoch [19/150], Loss: 0.3021637596550204, Val Loss: 0.2455323189496994\n",
      "Epoch [20/150], Loss: 0.30178744072217567, Val Loss: 0.24532969295978546\n",
      "Epoch [21/150], Loss: 0.30017573055635505, Val Loss: 0.24513448774814606\n",
      "Epoch [22/150], Loss: 0.30032325235090024, Val Loss: 0.24494194984436035\n",
      "Epoch [23/150], Loss: 0.29956477604595555, Val Loss: 0.24475283920764923\n",
      "Epoch [24/150], Loss: 0.29911473176424513, Val Loss: 0.24456363916397095\n",
      "Epoch [25/150], Loss: 0.29758065200700723, Val Loss: 0.24438032507896423\n",
      "Epoch [26/150], Loss: 0.2983453672001455, Val Loss: 0.2441944181919098\n",
      "Epoch [27/150], Loss: 0.29881375596665266, Val Loss: 0.24401327967643738\n",
      "Epoch [28/150], Loss: 0.29738234220301063, Val Loss: 0.24383670091629028\n",
      "Epoch [29/150], Loss: 0.2959643051748709, Val Loss: 0.24366416037082672\n",
      "Epoch [30/150], Loss: 0.29703035483242207, Val Loss: 0.2434931993484497\n",
      "Epoch [31/150], Loss: 0.2968049072528715, Val Loss: 0.24332737922668457\n",
      "Epoch [32/150], Loss: 0.29592662573273665, Val Loss: 0.24316085875034332\n",
      "Epoch [33/150], Loss: 0.2961950793879915, Val Loss: 0.24300061166286469\n",
      "Epoch [34/150], Loss: 0.2947512931906495, Val Loss: 0.2428441196680069\n",
      "Epoch [35/150], Loss: 0.29396384077383875, Val Loss: 0.24268992245197296\n",
      "Epoch [36/150], Loss: 0.29407110831366395, Val Loss: 0.24253928661346436\n",
      "Epoch [37/150], Loss: 0.2938149986450367, Val Loss: 0.24238531291484833\n",
      "Epoch [38/150], Loss: 0.29391901000877313, Val Loss: 0.24223703145980835\n",
      "Epoch [39/150], Loss: 0.2940326401950651, Val Loss: 0.24209225177764893\n",
      "Epoch [40/150], Loss: 0.2928391741764433, Val Loss: 0.24195145070552826\n",
      "Epoch [41/150], Loss: 0.29221311554022766, Val Loss: 0.24180868268013\n",
      "Epoch [42/150], Loss: 0.291394028830219, Val Loss: 0.24167127907276154\n",
      "Epoch [43/150], Loss: 0.2909142288823067, Val Loss: 0.24153584241867065\n",
      "Epoch [44/150], Loss: 0.29162857476577936, Val Loss: 0.2414020150899887\n",
      "Epoch [45/150], Loss: 0.290733532633035, Val Loss: 0.24126958847045898\n",
      "Epoch [46/150], Loss: 0.2911957872082602, Val Loss: 0.24114300310611725\n",
      "Epoch [47/150], Loss: 0.2907764480431687, Val Loss: 0.2410145401954651\n",
      "Epoch [48/150], Loss: 0.28974013628640194, Val Loss: 0.24089179933071136\n",
      "Epoch [49/150], Loss: 0.2900239546708587, Val Loss: 0.24077081680297852\n",
      "Epoch [50/150], Loss: 0.2894444326641706, Val Loss: 0.24065221846103668\n",
      "Epoch [51/150], Loss: 0.28948261047810103, Val Loss: 0.24053412675857544\n",
      "Epoch [52/150], Loss: 0.2891883442490274, Val Loss: 0.24041706323623657\n",
      "Epoch [53/150], Loss: 0.28866901162150754, Val Loss: 0.2403019517660141\n",
      "Epoch [54/150], Loss: 0.2885111652771538, Val Loss: 0.24018633365631104\n",
      "Epoch [55/150], Loss: 0.2878931459535862, Val Loss: 0.24007678031921387\n",
      "Epoch [56/150], Loss: 0.2868098389436403, Val Loss: 0.23996727168560028\n",
      "Epoch [57/150], Loss: 0.2877819366568594, Val Loss: 0.23985910415649414\n",
      "Epoch [58/150], Loss: 0.28758794557575557, Val Loss: 0.239752396941185\n",
      "Epoch [59/150], Loss: 0.2872394369058487, Val Loss: 0.23964960873126984\n",
      "Epoch [60/150], Loss: 0.28703241210369357, Val Loss: 0.23954841494560242\n",
      "Epoch [61/150], Loss: 0.28732635210628427, Val Loss: 0.23944701254367828\n",
      "Epoch [62/150], Loss: 0.2864517233189512, Val Loss: 0.23934975266456604\n",
      "Epoch [63/150], Loss: 0.2863061560212441, Val Loss: 0.23925219476222992\n",
      "Epoch [64/150], Loss: 0.28606078800314794, Val Loss: 0.23915758728981018\n",
      "Epoch [65/150], Loss: 0.2851119424646966, Val Loss: 0.2390657663345337\n",
      "Epoch [66/150], Loss: 0.2846914940329163, Val Loss: 0.23897193372249603\n",
      "Epoch [67/150], Loss: 0.2858313287120699, Val Loss: 0.23888243734836578\n",
      "Epoch [68/150], Loss: 0.2849776117276084, Val Loss: 0.23879408836364746\n",
      "Epoch [69/150], Loss: 0.2843430490773347, Val Loss: 0.2387072592973709\n",
      "Epoch [70/150], Loss: 0.28467166431705065, Val Loss: 0.23862230777740479\n",
      "Epoch [71/150], Loss: 0.2839919955047937, Val Loss: 0.23853932321071625\n",
      "Epoch [72/150], Loss: 0.28403065390753085, Val Loss: 0.2384573072195053\n",
      "Epoch [73/150], Loss: 0.28361012570709343, Val Loss: 0.23837536573410034\n",
      "Epoch [74/150], Loss: 0.2836967557932268, Val Loss: 0.23829637467861176\n",
      "Epoch [75/150], Loss: 0.2828914637607083, Val Loss: 0.23822055757045746\n",
      "Epoch [76/150], Loss: 0.2830715070913528, Val Loss: 0.23814354836940765\n",
      "Epoch [77/150], Loss: 0.2831073744257666, Val Loss: 0.238067626953125\n",
      "Epoch [78/150], Loss: 0.2831078187281371, Val Loss: 0.2379947304725647\n",
      "Epoch [79/150], Loss: 0.2819322991593442, Val Loss: 0.23792077600955963\n",
      "Epoch [80/150], Loss: 0.28259377366915905, Val Loss: 0.23785118758678436\n",
      "Epoch [81/150], Loss: 0.28233344398025984, Val Loss: 0.23778221011161804\n",
      "Epoch [82/150], Loss: 0.2823126869526569, Val Loss: 0.23771558701992035\n",
      "Epoch [83/150], Loss: 0.28194840781759684, Val Loss: 0.23764681816101074\n",
      "Epoch [84/150], Loss: 0.2821493212590521, Val Loss: 0.23758096992969513\n",
      "Epoch [85/150], Loss: 0.28120942595019444, Val Loss: 0.23751631379127502\n",
      "Epoch [86/150], Loss: 0.2818546561098148, Val Loss: 0.2374538779258728\n",
      "Epoch [87/150], Loss: 0.28087743002820303, Val Loss: 0.23738934099674225\n",
      "Epoch [88/150], Loss: 0.2822207977099098, Val Loss: 0.23733019828796387\n",
      "Epoch [89/150], Loss: 0.28059284206049273, Val Loss: 0.23726972937583923\n",
      "Epoch [90/150], Loss: 0.2805803166614529, Val Loss: 0.23720993101596832\n",
      "Epoch [91/150], Loss: 0.2804097193776507, Val Loss: 0.23715347051620483\n",
      "Epoch [92/150], Loss: 0.28035913066903373, Val Loss: 0.23709620535373688\n",
      "Epoch [93/150], Loss: 0.2802370377565857, Val Loss: 0.23704174160957336\n",
      "Epoch [94/150], Loss: 0.2795181038925635, Val Loss: 0.23698489367961884\n",
      "Epoch [95/150], Loss: 0.2795651599321128, Val Loss: 0.23693035542964935\n",
      "Epoch [96/150], Loss: 0.2796818974739784, Val Loss: 0.23687684535980225\n",
      "Epoch [97/150], Loss: 0.279749349227509, Val Loss: 0.23682570457458496\n",
      "Epoch [98/150], Loss: 0.2794409912026856, Val Loss: 0.23677517473697662\n",
      "Epoch [99/150], Loss: 0.27895022614613035, Val Loss: 0.23672686517238617\n",
      "Epoch [100/150], Loss: 0.2783379639907441, Val Loss: 0.2366783171892166\n",
      "Epoch [101/150], Loss: 0.2788014085128693, Val Loss: 0.23662975430488586\n",
      "Epoch [102/150], Loss: 0.2789254212874989, Val Loss: 0.23658424615859985\n",
      "Epoch [103/150], Loss: 0.27901804895043586, Val Loss: 0.2365400195121765\n",
      "Epoch [104/150], Loss: 0.27857705775015756, Val Loss: 0.23649640381336212\n",
      "Epoch [105/150], Loss: 0.27858874419449287, Val Loss: 0.23645205795764923\n",
      "Epoch [106/150], Loss: 0.2786711620975216, Val Loss: 0.23640944063663483\n",
      "Epoch [107/150], Loss: 0.27790754442850024, Val Loss: 0.23636646568775177\n",
      "Epoch [108/150], Loss: 0.2781658155650322, Val Loss: 0.23632502555847168\n",
      "Epoch [109/150], Loss: 0.2776372578469509, Val Loss: 0.236285001039505\n",
      "Epoch [110/150], Loss: 0.2781409923698894, Val Loss: 0.2362471967935562\n",
      "Epoch [111/150], Loss: 0.2779427193970648, Val Loss: 0.23621013760566711\n",
      "Epoch [112/150], Loss: 0.2778867286792517, Val Loss: 0.2361726313829422\n",
      "Epoch [113/150], Loss: 0.27732482118716567, Val Loss: 0.2361350804567337\n",
      "Epoch [114/150], Loss: 0.27781692304086153, Val Loss: 0.23609890043735504\n",
      "Epoch [115/150], Loss: 0.27749850196359716, Val Loss: 0.23606280982494354\n",
      "Epoch [116/150], Loss: 0.2771380840743593, Val Loss: 0.23602797091007233\n",
      "Epoch [117/150], Loss: 0.27725121568018607, Val Loss: 0.23599518835544586\n",
      "Epoch [118/150], Loss: 0.27685399317042025, Val Loss: 0.2359626740217209\n",
      "Epoch [119/150], Loss: 0.27625701585696694, Val Loss: 0.23593106865882874\n",
      "Epoch [120/150], Loss: 0.2772861683613814, Val Loss: 0.23590156435966492\n",
      "Epoch [121/150], Loss: 0.276939241404487, Val Loss: 0.23587222397327423\n",
      "Epoch [122/150], Loss: 0.2770026196497229, Val Loss: 0.23584339022636414\n",
      "Epoch [123/150], Loss: 0.27674907134222043, Val Loss: 0.2358141392469406\n",
      "Epoch [124/150], Loss: 0.2765237727874609, Val Loss: 0.23578515648841858\n",
      "Epoch [125/150], Loss: 0.27684406252832694, Val Loss: 0.23575827479362488\n",
      "Epoch [126/150], Loss: 0.27678286483102776, Val Loss: 0.2357316017150879\n",
      "Epoch [127/150], Loss: 0.27638926245667533, Val Loss: 0.23570550978183746\n",
      "Epoch [128/150], Loss: 0.2763043180775811, Val Loss: 0.23568019270896912\n",
      "Epoch [129/150], Loss: 0.2764868865180585, Val Loss: 0.2356557995080948\n",
      "Epoch [130/150], Loss: 0.27624252343436106, Val Loss: 0.23563246428966522\n",
      "Epoch [131/150], Loss: 0.27605846635821574, Val Loss: 0.2356095314025879\n",
      "Epoch [132/150], Loss: 0.2754375870704194, Val Loss: 0.23558512330055237\n",
      "Epoch [133/150], Loss: 0.27601719029766136, Val Loss: 0.2355642020702362\n",
      "Epoch [134/150], Loss: 0.2761887865303576, Val Loss: 0.2355436235666275\n",
      "Epoch [135/150], Loss: 0.2757499944857972, Val Loss: 0.2355230301618576\n",
      "Epoch [136/150], Loss: 0.27609688286089673, Val Loss: 0.23550382256507874\n",
      "Epoch [137/150], Loss: 0.27619559425456486, Val Loss: 0.23548416793346405\n",
      "Epoch [138/150], Loss: 0.2754857515595537, Val Loss: 0.23546521365642548\n",
      "Epoch [139/150], Loss: 0.27542780991204363, Val Loss: 0.2354450821876526\n",
      "Epoch [140/150], Loss: 0.27515655278913537, Val Loss: 0.2354254424571991\n",
      "Epoch [141/150], Loss: 0.2754157404217623, Val Loss: 0.23540815711021423\n",
      "Epoch [142/150], Loss: 0.2752811872193871, Val Loss: 0.23538972437381744\n",
      "Epoch [143/150], Loss: 0.27577468556712187, Val Loss: 0.23537445068359375\n",
      "Epoch [144/150], Loss: 0.2756205254150906, Val Loss: 0.23535889387130737\n",
      "Epoch [145/150], Loss: 0.27513139007529597, Val Loss: 0.23534198105335236\n",
      "Epoch [146/150], Loss: 0.27501486698033745, Val Loss: 0.23532576858997345\n",
      "Epoch [147/150], Loss: 0.27554760646417664, Val Loss: 0.23531222343444824\n",
      "Epoch [148/150], Loss: 0.2748824851587415, Val Loss: 0.2352980524301529\n",
      "Epoch [149/150], Loss: 0.2754341485154516, Val Loss: 0.23528432846069336\n",
      "Epoch [150/150], Loss: 0.2748773735398778, Val Loss: 0.2352709323167801\n",
      "MSE Loss: 0.2352709323167801\n",
      "Evaluating with parameters: {'lr': 4.767777567594389e-06, 'hidden_size': 162, 'num_layers': 2, 'dropout_rate_1': 0.4627224003896117, 'dropout_rate_2': 0.03532599927857518, 'dropout_rate_fc': 0.5893844901584089, 'weight_decay': 0.11834298043787758, 'batch_size': 15, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([1, 15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.37125684458259817, Val Loss: 0.27946433424949646\n",
      "Epoch [2/150], Loss: 0.36111093538430283, Val Loss: 0.2730114758014679\n",
      "Epoch [3/150], Loss: 0.3513003930638001, Val Loss: 0.26727038621902466\n",
      "Epoch [4/150], Loss: 0.3425038147909747, Val Loss: 0.2622385323047638\n",
      "Epoch [5/150], Loss: 0.3342928248429474, Val Loss: 0.2577550709247589\n",
      "Epoch [6/150], Loss: 0.32677591185639765, Val Loss: 0.2537767291069031\n",
      "Epoch [7/150], Loss: 0.3188858545927635, Val Loss: 0.2502041459083557\n",
      "Epoch [8/150], Loss: 0.3124085494564717, Val Loss: 0.2470034658908844\n",
      "Epoch [9/150], Loss: 0.30728780249417886, Val Loss: 0.24417079985141754\n",
      "Epoch [10/150], Loss: 0.3010216185679234, Val Loss: 0.24161942303180695\n",
      "Epoch [11/150], Loss: 0.29574935172024686, Val Loss: 0.2393299639225006\n",
      "Epoch [12/150], Loss: 0.2911330923220717, Val Loss: 0.2373025119304657\n",
      "Epoch [13/150], Loss: 0.2859675466744904, Val Loss: 0.23548923432826996\n",
      "Epoch [14/150], Loss: 0.28138686971368937, Val Loss: 0.233893021941185\n",
      "Epoch [15/150], Loss: 0.2773842925580044, Val Loss: 0.23250609636306763\n",
      "Epoch [16/150], Loss: 0.2734219705927948, Val Loss: 0.23128928244113922\n",
      "Epoch [17/150], Loss: 0.2696339744987287, Val Loss: 0.23023001849651337\n",
      "Epoch [18/150], Loss: 0.26645480204420025, Val Loss: 0.22931252419948578\n",
      "Epoch [19/150], Loss: 0.2625893680702366, Val Loss: 0.22852103412151337\n",
      "Epoch [20/150], Loss: 0.26074929194689483, Val Loss: 0.22786644101142883\n",
      "Epoch [21/150], Loss: 0.2579393520556655, Val Loss: 0.2273213267326355\n",
      "Epoch [22/150], Loss: 0.2561248925847235, Val Loss: 0.22687549889087677\n",
      "Epoch [23/150], Loss: 0.2518705618040203, Val Loss: 0.22650283575057983\n",
      "Epoch [24/150], Loss: 0.25049383922485674, Val Loss: 0.22621650993824005\n",
      "Epoch [25/150], Loss: 0.2487310922586463, Val Loss: 0.22599615156650543\n",
      "Epoch [26/150], Loss: 0.2471539781525945, Val Loss: 0.2258421629667282\n",
      "Epoch [27/150], Loss: 0.24434681731596972, Val Loss: 0.2257375419139862\n",
      "Epoch [28/150], Loss: 0.24365156749544772, Val Loss: 0.22568176686763763\n",
      "Epoch [29/150], Loss: 0.24181393945126814, Val Loss: 0.22566740214824677\n",
      "Epoch [30/150], Loss: 0.23999729451619292, Val Loss: 0.22568993270397186\n",
      "Epoch [31/150], Loss: 0.23806380848519876, Val Loss: 0.22574573755264282\n",
      "Epoch [32/150], Loss: 0.23771698427670762, Val Loss: 0.22582732141017914\n",
      "Epoch [33/150], Loss: 0.23584063749744913, Val Loss: 0.22593463957309723\n",
      "Epoch [34/150], Loss: 0.2350438813593807, Val Loss: 0.22606323659420013\n",
      "Epoch [35/150], Loss: 0.2344676146214811, Val Loss: 0.22620627284049988\n",
      "Epoch [36/150], Loss: 0.2329991638595013, Val Loss: 0.22636853158473969\n",
      "Epoch [37/150], Loss: 0.23217732701070742, Val Loss: 0.2265373021364212\n",
      "Epoch [38/150], Loss: 0.23144675932087433, Val Loss: 0.22672177851200104\n",
      "Epoch [39/150], Loss: 0.2308344666943168, Val Loss: 0.22691376507282257\n",
      "Early stopping at epoch 39\n",
      "MSE Loss: 0.22566740214824677\n",
      "Evaluating with parameters: {'lr': 5.069032972114682e-05, 'hidden_size': 145, 'num_layers': 3, 'dropout_rate_1': 0.04036653507500887, 'dropout_rate_2': 0.6937310586683452, 'dropout_rate_fc': 0.1939491002820432, 'weight_decay': 0.9554490315134506, 'batch_size': 55, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([55])) that is different to the input size (torch.Size([1, 55])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.28905487192569523, Val Loss: 0.24021539092063904\n",
      "Epoch [2/150], Loss: 0.2845450994343712, Val Loss: 0.23866596817970276\n",
      "Epoch [3/150], Loss: 0.2813841411229837, Val Loss: 0.2377292662858963\n",
      "Epoch [4/150], Loss: 0.27918714016757823, Val Loss: 0.23717951774597168\n",
      "Epoch [5/150], Loss: 0.2781475870870054, Val Loss: 0.236897274851799\n",
      "Epoch [6/150], Loss: 0.27759334287902976, Val Loss: 0.23679710924625397\n",
      "Epoch [7/150], Loss: 0.27748900693380735, Val Loss: 0.23680433630943298\n",
      "Epoch [8/150], Loss: 0.27758081948671204, Val Loss: 0.23687665164470673\n",
      "Epoch [9/150], Loss: 0.27772488343723856, Val Loss: 0.2369886338710785\n",
      "Epoch [10/150], Loss: 0.2779309972762488, Val Loss: 0.23711124062538147\n",
      "Epoch [11/150], Loss: 0.2782060839440469, Val Loss: 0.23722733557224274\n",
      "Epoch [12/150], Loss: 0.27848228132147546, Val Loss: 0.2373277246952057\n",
      "Epoch [13/150], Loss: 0.2787211753714543, Val Loss: 0.23740233480930328\n",
      "Epoch [14/150], Loss: 0.27887486468918704, Val Loss: 0.23744526505470276\n",
      "Epoch [15/150], Loss: 0.2789192335925751, Val Loss: 0.2374487817287445\n",
      "Epoch [16/150], Loss: 0.27886386974049465, Val Loss: 0.23741081357002258\n",
      "Early stopping at epoch 16\n",
      "MSE Loss: 0.23679710924625397\n",
      "Evaluating with parameters: {'lr': 0.0007032543625166421, 'hidden_size': 237, 'num_layers': 2, 'dropout_rate_1': 0.6131048150360584, 'dropout_rate_2': 0.4722218218259513, 'dropout_rate_fc': 0.03344439603388309, 'weight_decay': 0.29308682497870264, 'batch_size': 99, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([99])) that is different to the input size (torch.Size([1, 99])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([1, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.3041056174894466, Val Loss: 0.23206397891044617\n",
      "Epoch [2/150], Loss: 0.25181388913416397, Val Loss: 0.22732125222682953\n",
      "Epoch [3/150], Loss: 0.24077032026195644, Val Loss: 0.22648964822292328\n",
      "Epoch [4/150], Loss: 0.23752460898101038, Val Loss: 0.226273313164711\n",
      "Epoch [5/150], Loss: 0.23651801888812699, Val Loss: 0.22618164122104645\n",
      "Epoch [6/150], Loss: 0.2360085942417237, Val Loss: 0.22611969709396362\n",
      "Epoch [7/150], Loss: 0.23556091906685456, Val Loss: 0.22605863213539124\n",
      "Epoch [8/150], Loss: 0.23505211895441308, Val Loss: 0.22599932551383972\n",
      "Epoch [9/150], Loss: 0.23451458648139356, Val Loss: 0.22593896090984344\n",
      "Epoch [10/150], Loss: 0.23393963062770518, Val Loss: 0.22587929666042328\n",
      "Epoch [11/150], Loss: 0.23333137336314894, Val Loss: 0.22582097351551056\n",
      "Epoch [12/150], Loss: 0.23260608945480166, Val Loss: 0.22576184570789337\n",
      "Epoch [13/150], Loss: 0.2318624086842379, Val Loss: 0.22570879757404327\n",
      "Epoch [14/150], Loss: 0.23095123746487148, Val Loss: 0.22566117346286774\n",
      "Epoch [15/150], Loss: 0.22998187993196587, Val Loss: 0.22562643885612488\n",
      "Epoch [16/150], Loss: 0.22885305877300163, Val Loss: 0.2256099283695221\n",
      "Epoch [17/150], Loss: 0.22766095065676115, Val Loss: 0.22562001645565033\n",
      "Epoch [18/150], Loss: 0.2263056437813622, Val Loss: 0.22566691040992737\n",
      "Epoch [19/150], Loss: 0.22482165224010162, Val Loss: 0.22576288878917694\n",
      "Epoch [20/150], Loss: 0.22328212900160282, Val Loss: 0.22591812908649445\n",
      "Epoch [21/150], Loss: 0.22165528160757295, Val Loss: 0.22614482045173645\n",
      "Epoch [22/150], Loss: 0.21998416231579931, Val Loss: 0.2264508605003357\n",
      "Epoch [23/150], Loss: 0.21830591376797825, Val Loss: 0.22683976590633392\n",
      "Epoch [24/150], Loss: 0.2166515901757806, Val Loss: 0.2273104041814804\n",
      "Epoch [25/150], Loss: 0.2150698951536826, Val Loss: 0.2278529405593872\n",
      "Epoch [26/150], Loss: 0.21357349489870317, Val Loss: 0.2284558117389679\n",
      "Early stopping at epoch 26\n",
      "MSE Loss: 0.2256099283695221\n",
      "Evaluating with parameters: {'lr': 1.1470279965066186e-05, 'hidden_size': 69, 'num_layers': 3, 'dropout_rate_1': 0.19622084982693194, 'dropout_rate_2': 0.2568232511170208, 'dropout_rate_fc': 0.4303104761987924, 'weight_decay': 0.6321465996838883, 'batch_size': 51, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([51])) that is different to the input size (torch.Size([1, 51])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([1, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.3574807510369134, Val Loss: 0.2754947245121002\n",
      "Epoch [2/150], Loss: 0.3542831078342789, Val Loss: 0.27335888147354126\n",
      "Epoch [3/150], Loss: 0.35167005877675755, Val Loss: 0.27136319875717163\n",
      "Epoch [4/150], Loss: 0.3480748681693661, Val Loss: 0.2695116698741913\n",
      "Epoch [5/150], Loss: 0.3445585267428233, Val Loss: 0.2677985429763794\n",
      "Epoch [6/150], Loss: 0.34154464325354417, Val Loss: 0.2661969065666199\n",
      "Epoch [7/150], Loss: 0.3395192026901914, Val Loss: 0.26469185948371887\n",
      "Epoch [8/150], Loss: 0.3365361247762886, Val Loss: 0.26329246163368225\n",
      "Epoch [9/150], Loss: 0.3338565143699549, Val Loss: 0.26198750734329224\n",
      "Epoch [10/150], Loss: 0.33120483512591037, Val Loss: 0.2607637047767639\n",
      "Epoch [11/150], Loss: 0.3289101721756921, Val Loss: 0.2596139907836914\n",
      "Epoch [12/150], Loss: 0.3272454316005567, Val Loss: 0.2585349380970001\n",
      "Epoch [13/150], Loss: 0.3251203812601767, Val Loss: 0.2575162351131439\n",
      "Epoch [14/150], Loss: 0.32288262953183483, Val Loss: 0.2565637528896332\n",
      "Epoch [15/150], Loss: 0.32134720689750146, Val Loss: 0.2556598484516144\n",
      "Epoch [16/150], Loss: 0.31948622832150786, Val Loss: 0.2548128366470337\n",
      "Epoch [17/150], Loss: 0.31791471889033457, Val Loss: 0.25400444865226746\n",
      "Epoch [18/150], Loss: 0.3160756306095543, Val Loss: 0.25323620438575745\n",
      "Epoch [19/150], Loss: 0.315136806597477, Val Loss: 0.25251495838165283\n",
      "Epoch [20/150], Loss: 0.3131898781189657, Val Loss: 0.25183653831481934\n",
      "Epoch [21/150], Loss: 0.31137326239532204, Val Loss: 0.2511870265007019\n",
      "Epoch [22/150], Loss: 0.3109207889587827, Val Loss: 0.2505771815776825\n",
      "Epoch [23/150], Loss: 0.30899091171366827, Val Loss: 0.24999108910560608\n",
      "Epoch [24/150], Loss: 0.3077720215838707, Val Loss: 0.24943313002586365\n",
      "Epoch [25/150], Loss: 0.30716940528732173, Val Loss: 0.24890942871570587\n",
      "Epoch [26/150], Loss: 0.30605503745206003, Val Loss: 0.2484150528907776\n",
      "Epoch [27/150], Loss: 0.30500206207305347, Val Loss: 0.24793145060539246\n",
      "Epoch [28/150], Loss: 0.30384751952405337, Val Loss: 0.24748480319976807\n",
      "Epoch [29/150], Loss: 0.30313939149776586, Val Loss: 0.24705323576927185\n",
      "Epoch [30/150], Loss: 0.30219343689992567, Val Loss: 0.24664315581321716\n",
      "Epoch [31/150], Loss: 0.30111790907911345, Val Loss: 0.24625416100025177\n",
      "Epoch [32/150], Loss: 0.300187287557566, Val Loss: 0.24588100612163544\n",
      "Epoch [33/150], Loss: 0.29939003401122305, Val Loss: 0.24552306532859802\n",
      "Epoch [34/150], Loss: 0.2989093205530424, Val Loss: 0.2451813668012619\n",
      "Epoch [35/150], Loss: 0.2979219384272868, Val Loss: 0.2448599934577942\n",
      "Epoch [36/150], Loss: 0.2969858617882948, Val Loss: 0.24455000460147858\n",
      "Epoch [37/150], Loss: 0.2965298864553321, Val Loss: 0.24424971640110016\n",
      "Epoch [38/150], Loss: 0.29591298125665255, Val Loss: 0.24397017061710358\n",
      "Epoch [39/150], Loss: 0.29500247307159766, Val Loss: 0.2437022477388382\n",
      "Epoch [40/150], Loss: 0.2949277500821544, Val Loss: 0.24344491958618164\n",
      "Epoch [41/150], Loss: 0.29417411056679804, Val Loss: 0.24320122599601746\n",
      "Epoch [42/150], Loss: 0.29354135155183625, Val Loss: 0.242965430021286\n",
      "Epoch [43/150], Loss: 0.2930836926893882, Val Loss: 0.2427402138710022\n",
      "Epoch [44/150], Loss: 0.29221689281034835, Val Loss: 0.2425282597541809\n",
      "Epoch [45/150], Loss: 0.2917787731834212, Val Loss: 0.2423212081193924\n",
      "Epoch [46/150], Loss: 0.2913548172646373, Val Loss: 0.24212540686130524\n",
      "Epoch [47/150], Loss: 0.29118525678510493, Val Loss: 0.24194520711898804\n",
      "Epoch [48/150], Loss: 0.29044009831838535, Val Loss: 0.24176566302776337\n",
      "Epoch [49/150], Loss: 0.29033022761649013, Val Loss: 0.2415989488363266\n",
      "Epoch [50/150], Loss: 0.2896222997414024, Val Loss: 0.2414351850748062\n",
      "Epoch [51/150], Loss: 0.2893680789085029, Val Loss: 0.24128107726573944\n",
      "Epoch [52/150], Loss: 0.28891413938966864, Val Loss: 0.24113203585147858\n",
      "Epoch [53/150], Loss: 0.28855446440034677, Val Loss: 0.24099445343017578\n",
      "Epoch [54/150], Loss: 0.28826799694144606, Val Loss: 0.2408633828163147\n",
      "Epoch [55/150], Loss: 0.2879844676080749, Val Loss: 0.24073579907417297\n",
      "Epoch [56/150], Loss: 0.2876418642652202, Val Loss: 0.24061527848243713\n",
      "Epoch [57/150], Loss: 0.28766506944535947, Val Loss: 0.24050430953502655\n",
      "Epoch [58/150], Loss: 0.28718185137330116, Val Loss: 0.24039527773857117\n",
      "Epoch [59/150], Loss: 0.28687850478323823, Val Loss: 0.24029405415058136\n",
      "Epoch [60/150], Loss: 0.28680622353151974, Val Loss: 0.2401987910270691\n",
      "Epoch [61/150], Loss: 0.286539258751353, Val Loss: 0.24010929465293884\n",
      "Epoch [62/150], Loss: 0.28627064246779343, Val Loss: 0.2400234490633011\n",
      "Epoch [63/150], Loss: 0.2861210686482527, Val Loss: 0.2399422824382782\n",
      "Epoch [64/150], Loss: 0.2856886679929092, Val Loss: 0.23986311256885529\n",
      "Epoch [65/150], Loss: 0.28559157288424214, Val Loss: 0.23978950083255768\n",
      "Epoch [66/150], Loss: 0.28549608066012816, Val Loss: 0.23971988260746002\n",
      "Epoch [67/150], Loss: 0.28529311749342906, Val Loss: 0.23965443670749664\n",
      "Epoch [68/150], Loss: 0.2849814600776881, Val Loss: 0.23959165811538696\n",
      "Epoch [69/150], Loss: 0.28499752570333303, Val Loss: 0.23953315615653992\n",
      "Epoch [70/150], Loss: 0.2848187521009763, Val Loss: 0.23947876691818237\n",
      "Epoch [71/150], Loss: 0.28477105183992535, Val Loss: 0.23942831158638\n",
      "Epoch [72/150], Loss: 0.2846187064067784, Val Loss: 0.23938068747520447\n",
      "Epoch [73/150], Loss: 0.2845228049495466, Val Loss: 0.23933649063110352\n",
      "Epoch [74/150], Loss: 0.28427061537394716, Val Loss: 0.23929312825202942\n",
      "Epoch [75/150], Loss: 0.2842807066959462, Val Loss: 0.23925448954105377\n",
      "Epoch [76/150], Loss: 0.28410979377210366, Val Loss: 0.23921680450439453\n",
      "Epoch [77/150], Loss: 0.28408254583945913, Val Loss: 0.23918163776397705\n",
      "Epoch [78/150], Loss: 0.28375584950518545, Val Loss: 0.23914720118045807\n",
      "Epoch [79/150], Loss: 0.2839002936261193, Val Loss: 0.23911671340465546\n",
      "Epoch [80/150], Loss: 0.2836876072901852, Val Loss: 0.2390865832567215\n",
      "Epoch [81/150], Loss: 0.2837742954891707, Val Loss: 0.239060178399086\n",
      "Epoch [82/150], Loss: 0.28357856494745204, Val Loss: 0.23903347551822662\n",
      "Epoch [83/150], Loss: 0.2834689091434892, Val Loss: 0.2390086054801941\n",
      "Epoch [84/150], Loss: 0.28351970008165783, Val Loss: 0.2389858365058899\n",
      "Epoch [85/150], Loss: 0.28331436941932353, Val Loss: 0.23896321654319763\n",
      "Epoch [86/150], Loss: 0.283340022127068, Val Loss: 0.23894377052783966\n",
      "Epoch [87/150], Loss: 0.28304976230601264, Val Loss: 0.2389211803674698\n",
      "Epoch [88/150], Loss: 0.2830451747179221, Val Loss: 0.23889996111392975\n",
      "Epoch [89/150], Loss: 0.28337488000338173, Val Loss: 0.2388845533132553\n",
      "Epoch [90/150], Loss: 0.2830625654872963, Val Loss: 0.23886743187904358\n",
      "Epoch [91/150], Loss: 0.28310962573254517, Val Loss: 0.2388516515493393\n",
      "Epoch [92/150], Loss: 0.2830618598037494, Val Loss: 0.2388359010219574\n",
      "Epoch [93/150], Loss: 0.2829416090864878, Val Loss: 0.23881936073303223\n",
      "Epoch [94/150], Loss: 0.2830571632010254, Val Loss: 0.23880541324615479\n",
      "Epoch [95/150], Loss: 0.2828684192797055, Val Loss: 0.23878902196884155\n",
      "Epoch [96/150], Loss: 0.28270470474048387, Val Loss: 0.2387717366218567\n",
      "Epoch [97/150], Loss: 0.28287146964861193, Val Loss: 0.23875771462917328\n",
      "Epoch [98/150], Loss: 0.28251482845920767, Val Loss: 0.23873867094516754\n",
      "Epoch [99/150], Loss: 0.2826742173994568, Val Loss: 0.2387227714061737\n",
      "Epoch [100/150], Loss: 0.2826415635930488, Val Loss: 0.238707035779953\n",
      "Epoch [101/150], Loss: 0.2825190825554143, Val Loss: 0.23868897557258606\n",
      "Epoch [102/150], Loss: 0.2824407817026106, Val Loss: 0.23867137730121613\n",
      "Epoch [103/150], Loss: 0.28246430859055216, Val Loss: 0.23865434527397156\n",
      "Epoch [104/150], Loss: 0.2823794960490979, Val Loss: 0.2386353760957718\n",
      "Epoch [105/150], Loss: 0.28234691431802905, Val Loss: 0.23861676454544067\n",
      "Epoch [106/150], Loss: 0.2823123360689425, Val Loss: 0.23859719932079315\n",
      "Epoch [107/150], Loss: 0.2822416561717472, Val Loss: 0.23857642710208893\n",
      "Epoch [108/150], Loss: 0.2822560141129153, Val Loss: 0.23855622112751007\n",
      "Epoch [109/150], Loss: 0.28215395645190944, Val Loss: 0.23853415250778198\n",
      "Epoch [110/150], Loss: 0.2820841948126386, Val Loss: 0.238510861992836\n",
      "Epoch [111/150], Loss: 0.2820263407050575, Val Loss: 0.2384870946407318\n",
      "Epoch [112/150], Loss: 0.2819160454293561, Val Loss: 0.2384611815214157\n",
      "Epoch [113/150], Loss: 0.28190850258130123, Val Loss: 0.23843523859977722\n",
      "Epoch [114/150], Loss: 0.28186198179515043, Val Loss: 0.2384083867073059\n",
      "Epoch [115/150], Loss: 0.281707716264705, Val Loss: 0.2383786141872406\n",
      "Epoch [116/150], Loss: 0.2816411847277183, Val Loss: 0.2383483350276947\n",
      "Epoch [117/150], Loss: 0.2815831524239169, Val Loss: 0.23831698298454285\n",
      "Epoch [118/150], Loss: 0.28142613387067933, Val Loss: 0.2382832169532776\n",
      "Epoch [119/150], Loss: 0.28138125452393553, Val Loss: 0.23824885487556458\n",
      "Epoch [120/150], Loss: 0.2812614174683255, Val Loss: 0.2382122427225113\n",
      "Epoch [121/150], Loss: 0.28117262279586297, Val Loss: 0.23817473649978638\n",
      "Epoch [122/150], Loss: 0.2811185439088743, Val Loss: 0.23813652992248535\n",
      "Epoch [123/150], Loss: 0.28095141839359566, Val Loss: 0.23809555172920227\n",
      "Epoch [124/150], Loss: 0.28091719916342206, Val Loss: 0.23805472254753113\n",
      "Epoch [125/150], Loss: 0.2807689262506533, Val Loss: 0.2380117028951645\n",
      "Epoch [126/150], Loss: 0.28070586024930855, Val Loss: 0.23796804249286652\n",
      "Epoch [127/150], Loss: 0.2805475771013761, Val Loss: 0.23792198300361633\n",
      "Epoch [128/150], Loss: 0.28041474237724456, Val Loss: 0.23787443339824677\n",
      "Epoch [129/150], Loss: 0.28026982029301245, Val Loss: 0.23782488703727722\n",
      "Epoch [130/150], Loss: 0.28009473367199794, Val Loss: 0.23777349293231964\n",
      "Epoch [131/150], Loss: 0.2800126638553314, Val Loss: 0.23772169649600983\n",
      "Epoch [132/150], Loss: 0.2798867701084771, Val Loss: 0.23766863346099854\n",
      "Epoch [133/150], Loss: 0.2797692473740222, Val Loss: 0.23761478066444397\n",
      "Epoch [134/150], Loss: 0.2795982097729337, Val Loss: 0.23755881190299988\n",
      "Epoch [135/150], Loss: 0.2794301280661543, Val Loss: 0.23750074207782745\n",
      "Epoch [136/150], Loss: 0.27926849919532865, Val Loss: 0.23744137585163116\n",
      "Epoch [137/150], Loss: 0.2791531413938489, Val Loss: 0.23738151788711548\n",
      "Epoch [138/150], Loss: 0.27898261494036497, Val Loss: 0.23732002079486847\n",
      "Epoch [139/150], Loss: 0.2788132533659132, Val Loss: 0.23725709319114685\n",
      "Epoch [140/150], Loss: 0.27863337658464493, Val Loss: 0.23719243705272675\n",
      "Epoch [141/150], Loss: 0.2784545998806514, Val Loss: 0.23712646961212158\n",
      "Epoch [142/150], Loss: 0.2782982821298326, Val Loss: 0.23705968260765076\n",
      "Epoch [143/150], Loss: 0.2781273894016726, Val Loss: 0.2369917333126068\n",
      "Epoch [144/150], Loss: 0.2779133865058574, Val Loss: 0.23692232370376587\n",
      "Epoch [145/150], Loss: 0.2777580008521315, Val Loss: 0.2368520200252533\n",
      "Epoch [146/150], Loss: 0.2775617923340475, Val Loss: 0.2367808222770691\n",
      "Epoch [147/150], Loss: 0.27739208072366917, Val Loss: 0.2367086112499237\n",
      "Epoch [148/150], Loss: 0.27720429579732103, Val Loss: 0.2366355061531067\n",
      "Epoch [149/150], Loss: 0.27699274629026616, Val Loss: 0.23656105995178223\n",
      "Epoch [150/150], Loss: 0.27676302809695885, Val Loss: 0.23648518323898315\n",
      "MSE Loss: 0.23648518323898315\n",
      "Evaluating with parameters: {'lr': 1.6588569159380383e-05, 'hidden_size': 212, 'num_layers': 3, 'dropout_rate_1': 0.2826979384757578, 'dropout_rate_2': 0.2919628987088799, 'dropout_rate_fc': 0.6455639895051717, 'weight_decay': 0.450238024166801, 'batch_size': 82, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([82])) that is different to the input size (torch.Size([1, 82])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([38])) that is different to the input size (torch.Size([1, 38])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.39509963868643905, Val Loss: 0.29657483100891113\n",
      "Epoch [2/150], Loss: 0.38866520857774334, Val Loss: 0.2923291325569153\n",
      "Epoch [3/150], Loss: 0.38183348039623166, Val Loss: 0.28845322132110596\n",
      "Epoch [4/150], Loss: 0.3752982386952785, Val Loss: 0.284925639629364\n",
      "Epoch [5/150], Loss: 0.3707820794957339, Val Loss: 0.2817479372024536\n",
      "Epoch [6/150], Loss: 0.3650274879344907, Val Loss: 0.2788664698600769\n",
      "Epoch [7/150], Loss: 0.3601384802003864, Val Loss: 0.276231974363327\n",
      "Epoch [8/150], Loss: 0.355563896906669, Val Loss: 0.27383187413215637\n",
      "Epoch [9/150], Loss: 0.3515226973800874, Val Loss: 0.27163398265838623\n",
      "Epoch [10/150], Loss: 0.34815638973453983, Val Loss: 0.2695888876914978\n",
      "Epoch [11/150], Loss: 0.344980658108338, Val Loss: 0.26774686574935913\n",
      "Epoch [12/150], Loss: 0.341630703815427, Val Loss: 0.26601719856262207\n",
      "Epoch [13/150], Loss: 0.3382213467762607, Val Loss: 0.2644144892692566\n",
      "Epoch [14/150], Loss: 0.33551791282828713, Val Loss: 0.2629396319389343\n",
      "Epoch [15/150], Loss: 0.33270109108969814, Val Loss: 0.26156747341156006\n",
      "Epoch [16/150], Loss: 0.3301999735050514, Val Loss: 0.2602902948856354\n",
      "Epoch [17/150], Loss: 0.3285132299681179, Val Loss: 0.2590876817703247\n",
      "Epoch [18/150], Loss: 0.32592960566160134, Val Loss: 0.2579641342163086\n",
      "Epoch [19/150], Loss: 0.32366997522653124, Val Loss: 0.2569255530834198\n",
      "Epoch [20/150], Loss: 0.3218097081865932, Val Loss: 0.25595855712890625\n",
      "Epoch [21/150], Loss: 0.32025231402672705, Val Loss: 0.2550521194934845\n",
      "Epoch [22/150], Loss: 0.31803655770958444, Val Loss: 0.254191130399704\n",
      "Epoch [23/150], Loss: 0.316366545306366, Val Loss: 0.2533806562423706\n",
      "Epoch [24/150], Loss: 0.3148076377931188, Val Loss: 0.25262218713760376\n",
      "Epoch [25/150], Loss: 0.31371474559189844, Val Loss: 0.2519201934337616\n",
      "Epoch [26/150], Loss: 0.3119119894309122, Val Loss: 0.25125181674957275\n",
      "Epoch [27/150], Loss: 0.3101889526502031, Val Loss: 0.2506197988986969\n",
      "Epoch [28/150], Loss: 0.3098054015245594, Val Loss: 0.25003552436828613\n",
      "Epoch [29/150], Loss: 0.3087487740472692, Val Loss: 0.24948902428150177\n",
      "Epoch [30/150], Loss: 0.3078895155645785, Val Loss: 0.24897468090057373\n",
      "Epoch [31/150], Loss: 0.30601934868781294, Val Loss: 0.24848046898841858\n",
      "Epoch [32/150], Loss: 0.30547071709373935, Val Loss: 0.24801990389823914\n",
      "Epoch [33/150], Loss: 0.30403410618911025, Val Loss: 0.24757423996925354\n",
      "Epoch [34/150], Loss: 0.30337757754643435, Val Loss: 0.24716341495513916\n",
      "Epoch [35/150], Loss: 0.30239508148343835, Val Loss: 0.24677295982837677\n",
      "Epoch [36/150], Loss: 0.30176767755727296, Val Loss: 0.24641019105911255\n",
      "Epoch [37/150], Loss: 0.3005765786425012, Val Loss: 0.24606062471866608\n",
      "Epoch [38/150], Loss: 0.30019518641419096, Val Loss: 0.2457321137189865\n",
      "Epoch [39/150], Loss: 0.29916770560819594, Val Loss: 0.2454153150320053\n",
      "Epoch [40/150], Loss: 0.2984902006497637, Val Loss: 0.2451132982969284\n",
      "Epoch [41/150], Loss: 0.29803164739955645, Val Loss: 0.24483878910541534\n",
      "Epoch [42/150], Loss: 0.29747127857608874, Val Loss: 0.24457241594791412\n",
      "Epoch [43/150], Loss: 0.29713510111218594, Val Loss: 0.2443273663520813\n",
      "Epoch [44/150], Loss: 0.29593889528244244, Val Loss: 0.2440817505121231\n",
      "Epoch [45/150], Loss: 0.2957883965468309, Val Loss: 0.2438603937625885\n",
      "Epoch [46/150], Loss: 0.29504409502642076, Val Loss: 0.24364402890205383\n",
      "Epoch [47/150], Loss: 0.29490672152672637, Val Loss: 0.2434438318014145\n",
      "Epoch [48/150], Loss: 0.2942261631737967, Val Loss: 0.24325188994407654\n",
      "Epoch [49/150], Loss: 0.29357281486030484, Val Loss: 0.2430676519870758\n",
      "Epoch [50/150], Loss: 0.29362854272982136, Val Loss: 0.24289579689502716\n",
      "Epoch [51/150], Loss: 0.2932752644734793, Val Loss: 0.24273686110973358\n",
      "Epoch [52/150], Loss: 0.292831560657894, Val Loss: 0.24258314073085785\n",
      "Epoch [53/150], Loss: 0.29262994474074877, Val Loss: 0.24243764579296112\n",
      "Epoch [54/150], Loss: 0.2927633099807579, Val Loss: 0.24230428040027618\n",
      "Epoch [55/150], Loss: 0.2914987226306904, Val Loss: 0.24216653406620026\n",
      "Epoch [56/150], Loss: 0.2917064981321331, Val Loss: 0.242045596241951\n",
      "Epoch [57/150], Loss: 0.2911311920243697, Val Loss: 0.24192577600479126\n",
      "Epoch [58/150], Loss: 0.2906413718019841, Val Loss: 0.24180731177330017\n",
      "Epoch [59/150], Loss: 0.29044684105109975, Val Loss: 0.24169419705867767\n",
      "Epoch [60/150], Loss: 0.2903563130463733, Val Loss: 0.24158833920955658\n",
      "Epoch [61/150], Loss: 0.29031683944287845, Val Loss: 0.2414916455745697\n",
      "Epoch [62/150], Loss: 0.28990332107441347, Val Loss: 0.24139632284641266\n",
      "Epoch [63/150], Loss: 0.28956845161489775, Val Loss: 0.24130558967590332\n",
      "Epoch [64/150], Loss: 0.2892989597787134, Val Loss: 0.2412104457616806\n",
      "Epoch [65/150], Loss: 0.28958905228703724, Val Loss: 0.24113567173480988\n",
      "Epoch [66/150], Loss: 0.2892622292713552, Val Loss: 0.24105952680110931\n",
      "Epoch [67/150], Loss: 0.2889935270745735, Val Loss: 0.24098481237888336\n",
      "Epoch [68/150], Loss: 0.2882384971455961, Val Loss: 0.2409079670906067\n",
      "Epoch [69/150], Loss: 0.28825406977509865, Val Loss: 0.2408335953950882\n",
      "Epoch [70/150], Loss: 0.28854606355555723, Val Loss: 0.24077005684375763\n",
      "Epoch [71/150], Loss: 0.2884539144083125, Val Loss: 0.24071088433265686\n",
      "Epoch [72/150], Loss: 0.2876811753653112, Val Loss: 0.24064621329307556\n",
      "Epoch [73/150], Loss: 0.2877492645724875, Val Loss: 0.24058492481708527\n",
      "Epoch [74/150], Loss: 0.28798597089213424, Val Loss: 0.24053218960762024\n",
      "Epoch [75/150], Loss: 0.2873744810702371, Val Loss: 0.24047233164310455\n",
      "Epoch [76/150], Loss: 0.2871905909087814, Val Loss: 0.24041643738746643\n",
      "Epoch [77/150], Loss: 0.2874292755407877, Val Loss: 0.2403668761253357\n",
      "Epoch [78/150], Loss: 0.2869202607906744, Val Loss: 0.24031652510166168\n",
      "Epoch [79/150], Loss: 0.28702629855299583, Val Loss: 0.24027040600776672\n",
      "Epoch [80/150], Loss: 0.28741339539162447, Val Loss: 0.24023284018039703\n",
      "Epoch [81/150], Loss: 0.28670579210290165, Val Loss: 0.24018463492393494\n",
      "Epoch [82/150], Loss: 0.28685029510591853, Val Loss: 0.2401416301727295\n",
      "Epoch [83/150], Loss: 0.2867374876666753, Val Loss: 0.24009965360164642\n",
      "Epoch [84/150], Loss: 0.2869140383344693, Val Loss: 0.24006211757659912\n",
      "Epoch [85/150], Loss: 0.2863803821936494, Val Loss: 0.2400209903717041\n",
      "Epoch [86/150], Loss: 0.28596308018218297, Val Loss: 0.23997581005096436\n",
      "Epoch [87/150], Loss: 0.2861284754376431, Val Loss: 0.2399338185787201\n",
      "Epoch [88/150], Loss: 0.2857089376718294, Val Loss: 0.23989169299602509\n",
      "Epoch [89/150], Loss: 0.28554699798954314, Val Loss: 0.23984797298908234\n",
      "Epoch [90/150], Loss: 0.2858277742186042, Val Loss: 0.23981067538261414\n",
      "Epoch [91/150], Loss: 0.2856988548377498, Val Loss: 0.2397744357585907\n",
      "Epoch [92/150], Loss: 0.2856324364232724, Val Loss: 0.23974035680294037\n",
      "Epoch [93/150], Loss: 0.285773225556143, Val Loss: 0.23970873653888702\n",
      "Epoch [94/150], Loss: 0.2850955034926778, Val Loss: 0.23966757953166962\n",
      "Epoch [95/150], Loss: 0.2850684538239338, Val Loss: 0.23962852358818054\n",
      "Epoch [96/150], Loss: 0.28527843298726396, Val Loss: 0.23959551751613617\n",
      "Epoch [97/150], Loss: 0.28534242727595277, Val Loss: 0.23956254124641418\n",
      "Epoch [98/150], Loss: 0.28545877032104083, Val Loss: 0.2395336925983429\n",
      "Epoch [99/150], Loss: 0.28545708450504015, Val Loss: 0.2395101636648178\n",
      "Epoch [100/150], Loss: 0.2851787198212792, Val Loss: 0.23947925865650177\n",
      "Epoch [101/150], Loss: 0.28509887964388386, Val Loss: 0.23945201933383942\n",
      "Epoch [102/150], Loss: 0.28464856059824833, Val Loss: 0.23941946029663086\n",
      "Epoch [103/150], Loss: 0.2850518071199538, Val Loss: 0.23939548432826996\n",
      "Epoch [104/150], Loss: 0.2842462753541157, Val Loss: 0.23935969173908234\n",
      "Epoch [105/150], Loss: 0.28462308254398283, Val Loss: 0.23932579159736633\n",
      "Epoch [106/150], Loss: 0.28441761452399317, Val Loss: 0.23929569125175476\n",
      "Epoch [107/150], Loss: 0.28458928121406524, Val Loss: 0.23926964402198792\n",
      "Epoch [108/150], Loss: 0.2842045121627753, Val Loss: 0.23923912644386292\n",
      "Epoch [109/150], Loss: 0.2840813100337982, Val Loss: 0.23920626938343048\n",
      "Epoch [110/150], Loss: 0.283988001099864, Val Loss: 0.23917469382286072\n",
      "Epoch [111/150], Loss: 0.2840901172918374, Val Loss: 0.2391476035118103\n",
      "Epoch [112/150], Loss: 0.2841247547967512, Val Loss: 0.23912161588668823\n",
      "Epoch [113/150], Loss: 0.28392797360410454, Val Loss: 0.23909488320350647\n",
      "Epoch [114/150], Loss: 0.28374890958676574, Val Loss: 0.2390640676021576\n",
      "Epoch [115/150], Loss: 0.28386362471052856, Val Loss: 0.23903615772724152\n",
      "Epoch [116/150], Loss: 0.28390665505020346, Val Loss: 0.23901303112506866\n",
      "Epoch [117/150], Loss: 0.2838877665153781, Val Loss: 0.23898889124393463\n",
      "Epoch [118/150], Loss: 0.28379974505085437, Val Loss: 0.2389669567346573\n",
      "Epoch [119/150], Loss: 0.283444220566603, Val Loss: 0.2389364391565323\n",
      "Epoch [120/150], Loss: 0.28336067207646176, Val Loss: 0.23890769481658936\n",
      "Epoch [121/150], Loss: 0.2835480362543317, Val Loss: 0.23888197541236877\n",
      "Epoch [122/150], Loss: 0.2832262255862111, Val Loss: 0.2388535439968109\n",
      "Epoch [123/150], Loss: 0.28331839458131397, Val Loss: 0.23882758617401123\n",
      "Epoch [124/150], Loss: 0.2833439604852532, Val Loss: 0.23880372941493988\n",
      "Epoch [125/150], Loss: 0.28341442590854204, Val Loss: 0.23878233134746552\n",
      "Epoch [126/150], Loss: 0.2829183377020183, Val Loss: 0.23875273764133453\n",
      "Epoch [127/150], Loss: 0.28286165686049425, Val Loss: 0.23872341215610504\n",
      "Epoch [128/150], Loss: 0.2826092534194716, Val Loss: 0.23868905007839203\n",
      "Epoch [129/150], Loss: 0.28279943947420744, Val Loss: 0.2386619746685028\n",
      "Epoch [130/150], Loss: 0.2832533571502713, Val Loss: 0.23864760994911194\n",
      "Epoch [131/150], Loss: 0.28272258273524337, Val Loss: 0.23862023651599884\n",
      "Epoch [132/150], Loss: 0.2825389978643812, Val Loss: 0.2385922372341156\n",
      "Epoch [133/150], Loss: 0.2824865998852937, Val Loss: 0.23856426775455475\n",
      "Epoch [134/150], Loss: 0.28250926210865623, Val Loss: 0.2385387271642685\n",
      "Epoch [135/150], Loss: 0.2826910412519193, Val Loss: 0.23851603269577026\n",
      "Epoch [136/150], Loss: 0.28257187913920057, Val Loss: 0.23849348723888397\n",
      "Epoch [137/150], Loss: 0.2823969958441668, Val Loss: 0.2384667694568634\n",
      "Epoch [138/150], Loss: 0.2822556387572015, Val Loss: 0.23843887448310852\n",
      "Epoch [139/150], Loss: 0.28222738817089893, Val Loss: 0.23841296136379242\n",
      "Epoch [140/150], Loss: 0.2820826850404016, Val Loss: 0.23838739097118378\n",
      "Epoch [141/150], Loss: 0.28183332470352535, Val Loss: 0.2383558452129364\n",
      "Epoch [142/150], Loss: 0.2822708704921066, Val Loss: 0.23833346366882324\n",
      "Epoch [143/150], Loss: 0.28133409836741746, Val Loss: 0.23829908668994904\n",
      "Epoch [144/150], Loss: 0.281476264850038, Val Loss: 0.23826678097248077\n",
      "Epoch [145/150], Loss: 0.28132724450504193, Val Loss: 0.238234281539917\n",
      "Epoch [146/150], Loss: 0.28159065169022707, Val Loss: 0.23820847272872925\n",
      "Epoch [147/150], Loss: 0.28138243144408603, Val Loss: 0.23817989230155945\n",
      "Epoch [148/150], Loss: 0.2812068007092495, Val Loss: 0.23814937472343445\n",
      "Epoch [149/150], Loss: 0.2813880544644399, Val Loss: 0.2381247878074646\n",
      "Epoch [150/150], Loss: 0.2812657271069093, Val Loss: 0.23809780180454254\n",
      "MSE Loss: 0.23809780180454254\n",
      "Evaluating with parameters: {'lr': 0.0001804240009398285, 'hidden_size': 93, 'num_layers': 2, 'dropout_rate_1': 0.5266303484328091, 'dropout_rate_2': 0.4254530683159828, 'dropout_rate_fc': 0.3430174496024847, 'weight_decay': 0.5998715363769754, 'batch_size': 34, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([34])) that is different to the input size (torch.Size([1, 34])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([1, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.3754686903741134, Val Loss: 0.27048996090888977\n",
      "Epoch [2/150], Loss: 0.3378645782008104, Val Loss: 0.2595643699169159\n",
      "Epoch [3/150], Loss: 0.32509363224500254, Val Loss: 0.2555959224700928\n",
      "Epoch [4/150], Loss: 0.3189807582825218, Val Loss: 0.25338926911354065\n",
      "Epoch [5/150], Loss: 0.3148103843264486, Val Loss: 0.25147783756256104\n",
      "Epoch [6/150], Loss: 0.3108145431519728, Val Loss: 0.24945133924484253\n",
      "Epoch [7/150], Loss: 0.3062670915323782, Val Loss: 0.24719543755054474\n",
      "Epoch [8/150], Loss: 0.30115477736299373, Val Loss: 0.2447490245103836\n",
      "Epoch [9/150], Loss: 0.2956116518216233, Val Loss: 0.24222515523433685\n",
      "Epoch [10/150], Loss: 0.289716462258645, Val Loss: 0.23971936106681824\n",
      "Epoch [11/150], Loss: 0.28370096732556105, Val Loss: 0.23732270300388336\n",
      "Epoch [12/150], Loss: 0.27775701441425726, Val Loss: 0.23511400818824768\n",
      "Epoch [13/150], Loss: 0.27199914880026743, Val Loss: 0.2331455796957016\n",
      "Epoch [14/150], Loss: 0.26653355072700574, Val Loss: 0.23144029080867767\n",
      "Epoch [15/150], Loss: 0.26144097425037205, Val Loss: 0.23000246286392212\n",
      "Epoch [16/150], Loss: 0.2567481695101652, Val Loss: 0.22881858050823212\n",
      "Epoch [17/150], Loss: 0.25246303448770896, Val Loss: 0.22786690294742584\n",
      "Epoch [18/150], Loss: 0.2485671768507965, Val Loss: 0.22712114453315735\n",
      "Epoch [19/150], Loss: 0.24503923900671024, Val Loss: 0.22655481100082397\n",
      "Epoch [20/150], Loss: 0.2418500768409066, Val Loss: 0.22614258527755737\n",
      "Epoch [21/150], Loss: 0.23897005677102007, Val Loss: 0.2258613109588623\n",
      "Epoch [22/150], Loss: 0.23637052467619132, Val Loss: 0.22569018602371216\n",
      "Epoch [23/150], Loss: 0.23402473743134242, Val Loss: 0.22561083734035492\n",
      "Epoch [24/150], Loss: 0.23190813112010453, Val Loss: 0.22560708224773407\n",
      "Epoch [25/150], Loss: 0.22999830693740092, Val Loss: 0.22566480934619904\n",
      "Epoch [26/150], Loss: 0.22827493858584266, Val Loss: 0.22577179968357086\n",
      "Epoch [27/150], Loss: 0.2267195224133639, Val Loss: 0.22591747343540192\n",
      "Epoch [28/150], Loss: 0.22531537476277988, Val Loss: 0.22609280049800873\n",
      "Epoch [29/150], Loss: 0.22404738600654464, Val Loss: 0.22629007697105408\n",
      "Epoch [30/150], Loss: 0.2229020216857874, Val Loss: 0.22650283575057983\n",
      "Epoch [31/150], Loss: 0.22186688933527887, Val Loss: 0.22672566771507263\n",
      "Epoch [32/150], Loss: 0.2209309662390446, Val Loss: 0.22695402801036835\n",
      "Epoch [33/150], Loss: 0.22008435003804713, Val Loss: 0.22718431055545807\n",
      "Epoch [34/150], Loss: 0.2193180308497849, Val Loss: 0.22741349041461945\n",
      "Early stopping at epoch 34\n",
      "MSE Loss: 0.22560708224773407\n",
      "Evaluating with parameters: {'lr': 8.132270509078105e-05, 'hidden_size': 183, 'num_layers': 3, 'dropout_rate_1': 0.12683894811198115, 'dropout_rate_2': 0.5540522806346416, 'dropout_rate_fc': 0.15217968737706541, 'weight_decay': 0.1486648908956228, 'batch_size': 32, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([1, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([1, 30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.3192296041739023, Val Loss: 0.24088649451732635\n",
      "Epoch [2/150], Loss: 0.2750682629119124, Val Loss: 0.22954612970352173\n",
      "Epoch [3/150], Loss: 0.2510419258246979, Val Loss: 0.22605782747268677\n",
      "Epoch [4/150], Loss: 0.23709410199174477, Val Loss: 0.2257470339536667\n",
      "Epoch [5/150], Loss: 0.228907853111835, Val Loss: 0.22651952505111694\n",
      "Epoch [6/150], Loss: 0.22419257704468984, Val Loss: 0.2275024801492691\n",
      "Epoch [7/150], Loss: 0.22108845101939814, Val Loss: 0.22838759422302246\n",
      "Epoch [8/150], Loss: 0.21890989829065097, Val Loss: 0.22908934950828552\n",
      "Epoch [9/150], Loss: 0.21782379559121065, Val Loss: 0.22959372401237488\n",
      "Epoch [10/150], Loss: 0.21709655755830387, Val Loss: 0.22996830940246582\n",
      "Epoch [11/150], Loss: 0.2162699914972989, Val Loss: 0.23023676872253418\n",
      "Epoch [12/150], Loss: 0.2157152942471927, Val Loss: 0.23045334219932556\n",
      "Epoch [13/150], Loss: 0.21561526986019267, Val Loss: 0.2305585741996765\n",
      "Epoch [14/150], Loss: 0.21538227174671426, Val Loss: 0.23067139089107513\n",
      "Early stopping at epoch 14\n",
      "MSE Loss: 0.2257470339536667\n",
      "Evaluating with parameters: {'lr': 1.3683958875987251e-06, 'hidden_size': 124, 'num_layers': 2, 'dropout_rate_1': 0.3762472636066377, 'dropout_rate_2': 0.16339703369885683, 'dropout_rate_fc': 0.4559858522377908, 'weight_decay': 0.80025098517452, 'batch_size': 73, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\ax\\modelbridge\\cross_validation.py:463: UserWarning: Encountered exception in computing model fit quality: RandomModelBridge does not support prediction.\n",
      "  warn(\"Encountered exception in computing model fit quality: \" + str(e))\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([73])) that is different to the input size (torch.Size([1, 73])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([67])) that is different to the input size (torch.Size([1, 67])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.43938867727239783, Val Loss: 0.3245663046836853\n",
      "Epoch [2/150], Loss: 0.43873702749089505, Val Loss: 0.324272483587265\n",
      "Epoch [3/150], Loss: 0.4378850019797135, Val Loss: 0.3239773213863373\n",
      "Epoch [4/150], Loss: 0.43682836888440174, Val Loss: 0.323682963848114\n",
      "Epoch [5/150], Loss: 0.43675117351470844, Val Loss: 0.3233848214149475\n",
      "Epoch [6/150], Loss: 0.4359564622179331, Val Loss: 0.3230940103530884\n",
      "Epoch [7/150], Loss: 0.43637115754844513, Val Loss: 0.32280170917510986\n",
      "Epoch [8/150], Loss: 0.43600217673434494, Val Loss: 0.3225134313106537\n",
      "Epoch [9/150], Loss: 0.43492259199539307, Val Loss: 0.3222251236438751\n",
      "Epoch [10/150], Loss: 0.4343405004371615, Val Loss: 0.32193970680236816\n",
      "Epoch [11/150], Loss: 0.4347143950050368, Val Loss: 0.321659117937088\n",
      "Epoch [12/150], Loss: 0.4333549762651434, Val Loss: 0.3213786482810974\n",
      "Epoch [13/150], Loss: 0.43372963564744327, Val Loss: 0.3210982382297516\n",
      "Epoch [14/150], Loss: 0.4330263484872001, Val Loss: 0.32082244753837585\n",
      "Epoch [15/150], Loss: 0.43205501445267785, Val Loss: 0.3205471932888031\n",
      "Epoch [16/150], Loss: 0.4311563910998623, Val Loss: 0.3202752470970154\n",
      "Epoch [17/150], Loss: 0.4322774532894768, Val Loss: 0.3200039863586426\n",
      "Epoch [18/150], Loss: 0.43190048587755026, Val Loss: 0.3197344243526459\n",
      "Epoch [19/150], Loss: 0.43052371533121914, Val Loss: 0.3194673955440521\n",
      "Epoch [20/150], Loss: 0.4304753397796851, Val Loss: 0.31919801235198975\n",
      "Epoch [21/150], Loss: 0.4298352034050314, Val Loss: 0.31893473863601685\n",
      "Epoch [22/150], Loss: 0.4297885386108914, Val Loss: 0.31867045164108276\n",
      "Epoch [23/150], Loss: 0.4295967937008861, Val Loss: 0.3184092342853546\n",
      "Epoch [24/150], Loss: 0.428206460796954, Val Loss: 0.3181503713130951\n",
      "Epoch [25/150], Loss: 0.4286240044132094, Val Loss: 0.31789445877075195\n",
      "Epoch [26/150], Loss: 0.4286031147930771, Val Loss: 0.31763797998428345\n",
      "Epoch [27/150], Loss: 0.42775457712602527, Val Loss: 0.317382276058197\n",
      "Epoch [28/150], Loss: 0.42692033565767545, Val Loss: 0.31712910532951355\n",
      "Epoch [29/150], Loss: 0.4270380004446077, Val Loss: 0.316877543926239\n",
      "Epoch [30/150], Loss: 0.4259771568741759, Val Loss: 0.31662824749946594\n",
      "Epoch [31/150], Loss: 0.4262588030508007, Val Loss: 0.31637972593307495\n",
      "Epoch [32/150], Loss: 0.4255740426494466, Val Loss: 0.3161327838897705\n",
      "Epoch [33/150], Loss: 0.42505975796238465, Val Loss: 0.31588926911354065\n",
      "Epoch [34/150], Loss: 0.4254338141010307, Val Loss: 0.31564533710479736\n",
      "Epoch [35/150], Loss: 0.42450649341951835, Val Loss: 0.315403014421463\n",
      "Epoch [36/150], Loss: 0.42408591708825794, Val Loss: 0.31516268849372864\n",
      "Epoch [37/150], Loss: 0.42302915242993655, Val Loss: 0.3149232864379883\n",
      "Epoch [38/150], Loss: 0.4229142375098651, Val Loss: 0.3146873116493225\n",
      "Epoch [39/150], Loss: 0.42311778034576597, Val Loss: 0.3144511580467224\n",
      "Epoch [40/150], Loss: 0.4224098804406822, Val Loss: 0.31421563029289246\n",
      "Epoch [41/150], Loss: 0.42248008318026276, Val Loss: 0.3139810860157013\n",
      "Epoch [42/150], Loss: 0.4217748569927233, Val Loss: 0.3137492835521698\n",
      "Epoch [43/150], Loss: 0.42183599512859743, Val Loss: 0.3135204613208771\n",
      "Epoch [44/150], Loss: 0.4210067129392615, Val Loss: 0.31329068541526794\n",
      "Epoch [45/150], Loss: 0.42129662863033657, Val Loss: 0.3130619525909424\n",
      "Epoch [46/150], Loss: 0.4204246197580634, Val Loss: 0.31283384561538696\n",
      "Epoch [47/150], Loss: 0.4204428229782292, Val Loss: 0.3126092851161957\n",
      "Epoch [48/150], Loss: 0.41995391909800034, Val Loss: 0.3123857378959656\n",
      "Epoch [49/150], Loss: 0.4193165088757215, Val Loss: 0.31216317415237427\n",
      "Epoch [50/150], Loss: 0.4193186230601414, Val Loss: 0.31194156408309937\n",
      "Epoch [51/150], Loss: 0.4191919656415634, Val Loss: 0.31172212958335876\n",
      "Epoch [52/150], Loss: 0.4178675950422664, Val Loss: 0.3115019202232361\n",
      "Epoch [53/150], Loss: 0.41738093308830526, Val Loss: 0.3112845718860626\n",
      "Epoch [54/150], Loss: 0.41728563850526423, Val Loss: 0.3110668361186981\n",
      "Epoch [55/150], Loss: 0.4174936645089046, Val Loss: 0.31085219979286194\n",
      "Epoch [56/150], Loss: 0.41747664727325384, Val Loss: 0.3106367588043213\n",
      "Epoch [57/150], Loss: 0.41658981870312023, Val Loss: 0.3104255795478821\n",
      "Epoch [58/150], Loss: 0.41666301634327013, Val Loss: 0.31021520495414734\n",
      "Epoch [59/150], Loss: 0.4162699371055864, Val Loss: 0.31000521779060364\n",
      "Epoch [60/150], Loss: 0.4157456247656442, Val Loss: 0.30979475378990173\n",
      "Epoch [61/150], Loss: 0.4156421883004334, Val Loss: 0.3095872104167938\n",
      "Epoch [62/150], Loss: 0.415423314059701, Val Loss: 0.3093797564506531\n",
      "Epoch [63/150], Loss: 0.4141769586952732, Val Loss: 0.30917367339134216\n",
      "Epoch [64/150], Loss: 0.41424870260936375, Val Loss: 0.3089701235294342\n",
      "Epoch [65/150], Loss: 0.41405447391683564, Val Loss: 0.30876612663269043\n",
      "Epoch [66/150], Loss: 0.41345534150433894, Val Loss: 0.30856290459632874\n",
      "Epoch [67/150], Loss: 0.41353606401175697, Val Loss: 0.3083624541759491\n",
      "Epoch [68/150], Loss: 0.4130719602518879, Val Loss: 0.30816248059272766\n",
      "Epoch [69/150], Loss: 0.412944587509093, Val Loss: 0.3079645335674286\n",
      "Epoch [70/150], Loss: 0.41268563098000255, Val Loss: 0.3077659606933594\n",
      "Epoch [71/150], Loss: 0.4121577585636474, Val Loss: 0.3075718581676483\n",
      "Epoch [72/150], Loss: 0.41180605448179824, Val Loss: 0.3073771297931671\n",
      "Epoch [73/150], Loss: 0.41185602432062085, Val Loss: 0.307183176279068\n",
      "Epoch [74/150], Loss: 0.4116184240316643, Val Loss: 0.3069899082183838\n",
      "Epoch [75/150], Loss: 0.4105470041223966, Val Loss: 0.3068002760410309\n",
      "Epoch [76/150], Loss: 0.41094699903281734, Val Loss: 0.30660882592201233\n",
      "Epoch [77/150], Loss: 0.4098039137144737, Val Loss: 0.30642086267471313\n",
      "Epoch [78/150], Loss: 0.4103619573528276, Val Loss: 0.30623292922973633\n",
      "Epoch [79/150], Loss: 0.4093690385001109, Val Loss: 0.3060472011566162\n",
      "Epoch [80/150], Loss: 0.40925524305716593, Val Loss: 0.3058604896068573\n",
      "Epoch [81/150], Loss: 0.4098561870016377, Val Loss: 0.3056762218475342\n",
      "Epoch [82/150], Loss: 0.409499636878643, Val Loss: 0.3054904639720917\n",
      "Epoch [83/150], Loss: 0.4082210656008957, Val Loss: 0.3053077757358551\n",
      "Epoch [84/150], Loss: 0.40799581764868514, Val Loss: 0.30512532591819763\n",
      "Epoch [85/150], Loss: 0.4081498972167644, Val Loss: 0.30494412779808044\n",
      "Epoch [86/150], Loss: 0.4073203576536959, Val Loss: 0.30476540327072144\n",
      "Epoch [87/150], Loss: 0.4077583317480543, Val Loss: 0.3045869767665863\n",
      "Epoch [88/150], Loss: 0.40749131837476266, Val Loss: 0.3044092059135437\n",
      "Epoch [89/150], Loss: 0.406618066926432, Val Loss: 0.3042328953742981\n",
      "Epoch [90/150], Loss: 0.4071727370268063, Val Loss: 0.30405810475349426\n",
      "Epoch [91/150], Loss: 0.4061164231833947, Val Loss: 0.3038826584815979\n",
      "Epoch [92/150], Loss: 0.40612084371969104, Val Loss: 0.3037092685699463\n",
      "Epoch [93/150], Loss: 0.40510699454256716, Val Loss: 0.3035351037979126\n",
      "Epoch [94/150], Loss: 0.4049741279279046, Val Loss: 0.3033633828163147\n",
      "Epoch [95/150], Loss: 0.40500098132692713, Val Loss: 0.30319157242774963\n",
      "Epoch [96/150], Loss: 0.40483352480291884, Val Loss: 0.3030221164226532\n",
      "Epoch [97/150], Loss: 0.4044371536686359, Val Loss: 0.3028508126735687\n",
      "Epoch [98/150], Loss: 0.40485226965564136, Val Loss: 0.30268383026123047\n",
      "Epoch [99/150], Loss: 0.40445029065834687, Val Loss: 0.3025164306163788\n",
      "Epoch [100/150], Loss: 0.4038880751468241, Val Loss: 0.3023516833782196\n",
      "Epoch [101/150], Loss: 0.403135980178109, Val Loss: 0.30218666791915894\n",
      "Epoch [102/150], Loss: 0.4038179291631369, Val Loss: 0.30202484130859375\n",
      "Epoch [103/150], Loss: 0.40278232885141146, Val Loss: 0.3018621504306793\n",
      "Epoch [104/150], Loss: 0.40306685200673253, Val Loss: 0.3017010986804962\n",
      "Epoch [105/150], Loss: 0.4030091613120235, Val Loss: 0.3015393018722534\n",
      "Epoch [106/150], Loss: 0.40235131611937985, Val Loss: 0.3013792037963867\n",
      "Epoch [107/150], Loss: 0.4019997390534948, Val Loss: 0.3012193739414215\n",
      "Epoch [108/150], Loss: 0.40124558614950406, Val Loss: 0.30106136202812195\n",
      "Epoch [109/150], Loss: 0.401864513179616, Val Loss: 0.3009047508239746\n",
      "Epoch [110/150], Loss: 0.40146919627509575, Val Loss: 0.300749272108078\n",
      "Epoch [111/150], Loss: 0.4010079638922916, Val Loss: 0.3005935549736023\n",
      "Epoch [112/150], Loss: 0.4004329487078768, Val Loss: 0.30043935775756836\n",
      "Epoch [113/150], Loss: 0.40074278581339645, Val Loss: 0.3002876043319702\n",
      "Epoch [114/150], Loss: 0.4004359460189281, Val Loss: 0.3001345098018646\n",
      "Epoch [115/150], Loss: 0.39973823034533246, Val Loss: 0.2999820113182068\n",
      "Epoch [116/150], Loss: 0.4002785254132879, Val Loss: 0.2998320460319519\n",
      "Epoch [117/150], Loss: 0.39915873871787505, Val Loss: 0.2996813952922821\n",
      "Epoch [118/150], Loss: 0.39904107724535554, Val Loss: 0.299532413482666\n",
      "Epoch [119/150], Loss: 0.39893632720443695, Val Loss: 0.2993837594985962\n",
      "Epoch [120/150], Loss: 0.3986824925934129, Val Loss: 0.29923775792121887\n",
      "Epoch [121/150], Loss: 0.3987215346492389, Val Loss: 0.2990909516811371\n",
      "Epoch [122/150], Loss: 0.3988247194656116, Val Loss: 0.29894691705703735\n",
      "Epoch [123/150], Loss: 0.39825875599704247, Val Loss: 0.2988007962703705\n",
      "Epoch [124/150], Loss: 0.39816274582956207, Val Loss: 0.29865819215774536\n",
      "Epoch [125/150], Loss: 0.3978330682250945, Val Loss: 0.29851478338241577\n",
      "Epoch [126/150], Loss: 0.3969105680966202, Val Loss: 0.2983724772930145\n",
      "Epoch [127/150], Loss: 0.39735909402096536, Val Loss: 0.29823148250579834\n",
      "Epoch [128/150], Loss: 0.3967760743792443, Val Loss: 0.298091322183609\n",
      "Epoch [129/150], Loss: 0.3968372376060442, Val Loss: 0.29795175790786743\n",
      "Epoch [130/150], Loss: 0.39655156122684915, Val Loss: 0.2978135645389557\n",
      "Epoch [131/150], Loss: 0.3965386411668185, Val Loss: 0.29767414927482605\n",
      "Epoch [132/150], Loss: 0.39601933584986804, Val Loss: 0.29753753542900085\n",
      "Epoch [133/150], Loss: 0.3957607463166556, Val Loss: 0.29739999771118164\n",
      "Epoch [134/150], Loss: 0.3956414897213964, Val Loss: 0.2972632050514221\n",
      "Epoch [135/150], Loss: 0.39546811773770435, Val Loss: 0.2971280515193939\n",
      "Epoch [136/150], Loss: 0.3955321417957106, Val Loss: 0.29699471592903137\n",
      "Epoch [137/150], Loss: 0.39487255431766455, Val Loss: 0.29686179757118225\n",
      "Epoch [138/150], Loss: 0.39469740419265104, Val Loss: 0.2967301905155182\n",
      "Epoch [139/150], Loss: 0.3943432667155695, Val Loss: 0.296597957611084\n",
      "Epoch [140/150], Loss: 0.39441097119603963, Val Loss: 0.29646730422973633\n",
      "Epoch [141/150], Loss: 0.3945418006658335, Val Loss: 0.2963365316390991\n",
      "Epoch [142/150], Loss: 0.3938458765966489, Val Loss: 0.2962067127227783\n",
      "Epoch [143/150], Loss: 0.39385056210791364, Val Loss: 0.2960772216320038\n",
      "Epoch [144/150], Loss: 0.39368535001652644, Val Loss: 0.29595062136650085\n",
      "Epoch [145/150], Loss: 0.39294522980173285, Val Loss: 0.29582205414772034\n",
      "Epoch [146/150], Loss: 0.39286405749290304, Val Loss: 0.2956959307193756\n",
      "Epoch [147/150], Loss: 0.3926002421580693, Val Loss: 0.29556864500045776\n",
      "Epoch [148/150], Loss: 0.3926998521475231, Val Loss: 0.2954428791999817\n",
      "Epoch [149/150], Loss: 0.39187062628950703, Val Loss: 0.2953175902366638\n",
      "Epoch [150/150], Loss: 0.3920905169592622, Val Loss: 0.295194536447525\n",
      "MSE Loss: 0.295194536447525\n",
      "Evaluating with parameters: {'lr': 1e-06, 'hidden_size': 129, 'num_layers': 2, 'dropout_rate_1': 0.342229464170154, 'dropout_rate_2': 0.07970704252559446, 'dropout_rate_fc': 0.4123881962034013, 'weight_decay': 0.8835487140126591, 'batch_size': 83, 'num_epochs': 150}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([83])) that is different to the input size (torch.Size([1, 83])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\Marco Ligabue\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([61])) that is different to the input size (torch.Size([1, 61])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.36198062244802715, Val Loss: 0.27820777893066406\n",
      "Epoch [2/150], Loss: 0.3615946862846613, Val Loss: 0.2780826687812805\n",
      "Epoch [3/150], Loss: 0.36221623203406733, Val Loss: 0.2779569923877716\n",
      "Epoch [4/150], Loss: 0.3615015803525845, Val Loss: 0.2778315544128418\n",
      "Epoch [5/150], Loss: 0.36167225607981285, Val Loss: 0.27770593762397766\n",
      "Epoch [6/150], Loss: 0.3610603898142775, Val Loss: 0.27758118510246277\n",
      "Epoch [7/150], Loss: 0.36078667544449367, Val Loss: 0.27745571732521057\n",
      "Epoch [8/150], Loss: 0.3613309249592324, Val Loss: 0.27733203768730164\n",
      "Epoch [9/150], Loss: 0.3608045004929105, Val Loss: 0.2772078812122345\n",
      "Epoch [10/150], Loss: 0.36001117223252854, Val Loss: 0.2770856022834778\n",
      "Epoch [11/150], Loss: 0.3599764894073208, Val Loss: 0.2769635021686554\n",
      "Epoch [12/150], Loss: 0.3600929093857606, Val Loss: 0.27684152126312256\n",
      "Epoch [13/150], Loss: 0.35972173636158306, Val Loss: 0.27672079205513\n",
      "Epoch [14/150], Loss: 0.3595679118918876, Val Loss: 0.27659985423088074\n",
      "Epoch [15/150], Loss: 0.359577781924357, Val Loss: 0.27648040652275085\n",
      "Epoch [16/150], Loss: 0.3593290975938241, Val Loss: 0.27636048197746277\n",
      "Epoch [17/150], Loss: 0.359535807184875, Val Loss: 0.2762421667575836\n",
      "Epoch [18/150], Loss: 0.3587852628901601, Val Loss: 0.276123583316803\n",
      "Epoch [19/150], Loss: 0.3586352448910475, Val Loss: 0.27600613236427307\n",
      "Epoch [20/150], Loss: 0.35863560140132905, Val Loss: 0.2758885622024536\n",
      "Epoch [21/150], Loss: 0.35810470273718237, Val Loss: 0.2757716476917267\n",
      "Epoch [22/150], Loss: 0.3582790787642201, Val Loss: 0.2756558060646057\n",
      "Epoch [23/150], Loss: 0.3576040449862679, Val Loss: 0.27554118633270264\n",
      "Epoch [24/150], Loss: 0.357163002807647, Val Loss: 0.27542605996131897\n",
      "Epoch [25/150], Loss: 0.35731649833420914, Val Loss: 0.2753114104270935\n",
      "Epoch [26/150], Loss: 0.35704447409758966, Val Loss: 0.27519676089286804\n",
      "Epoch [27/150], Loss: 0.3569323670429488, Val Loss: 0.2750833332538605\n",
      "Epoch [28/150], Loss: 0.356349537614733, Val Loss: 0.2749706208705902\n",
      "Epoch [29/150], Loss: 0.3563551798152427, Val Loss: 0.2748584449291229\n",
      "Epoch [30/150], Loss: 0.35566866854205725, Val Loss: 0.27474668622016907\n",
      "Epoch [31/150], Loss: 0.35640664569412667, Val Loss: 0.27463576197624207\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
